{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"machine_shape":"hm","gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12416783,"sourceType":"datasetVersion","datasetId":7813603}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install phonemizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:35:42.693008Z","iopub.execute_input":"2025-07-22T08:35:42.693286Z","iopub.status.idle":"2025-07-22T08:35:46.757600Z","shell.execute_reply.started":"2025-07-22T08:35:42.693264Z","shell.execute_reply":"2025-07-22T08:35:46.756764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyaudio\n!pip install python-Levenshtein\n!pip install requests ipywidgets sounddevice soundfile\n!pip install sequence_align","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:35:46.758994Z","iopub.execute_input":"2025-07-22T08:35:46.759698Z","iopub.status.idle":"2025-07-22T08:36:04.913209Z","shell.execute_reply.started":"2025-07-22T08:35:46.759657Z","shell.execute_reply":"2025-07-22T08:36:04.912364Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt-get install espeak-ng","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:04.914408Z","iopub.execute_input":"2025-07-22T08:36:04.914639Z","iopub.status.idle":"2025-07-22T08:36:08.225784Z","shell.execute_reply.started":"2025-07-22T08:36:04.914614Z","shell.execute_reply":"2025-07-22T08:36:08.224722Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nimport torch\nimport librosa\nimport soundfile as sf\nimport numpy as np\n\n# Choose a model checkpoint\nmodel_name = \"facebook/wav2vec2-lv-60-espeak-cv-ft\" # Example: multilingual phoneme model\n\n# Load the processor and model\nprocessor = Wav2Vec2Processor.from_pretrained(model_name)\nmodel = Wav2Vec2ForCTC.from_pretrained(model_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:08.227041Z","iopub.execute_input":"2025-07-22T08:36:08.227372Z","iopub.status.idle":"2025-07-22T08:36:25.489664Z","shell.execute_reply.started":"2025-07-22T08:36:08.227331Z","shell.execute_reply":"2025-07-22T08:36:25.488997Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"audio_path = \"/kaggle/input/audiotest/Recording.wav\" # Replace with your audio file path\n\n# Load audio file\nspeech, sample_rate = librosa.load(audio_path, sr=None)\n\n\n# Resample to 16kHz if needed\nif sample_rate != 16000:\n    speech = librosa.resample(speech, orig_sr=sample_rate, target_sr=16000)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:25.492244Z","iopub.execute_input":"2025-07-22T08:36:25.492678Z","iopub.status.idle":"2025-07-22T08:36:28.032356Z","shell.execute_reply.started":"2025-07-22T08:36:25.492656Z","shell.execute_reply":"2025-07-22T08:36:28.031376Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare input values for the model\n# The `return_tensors=\"pt\"` argument ensures PyTorch tensors are returned.\ninput_values = processor(speech, sampling_rate=16000, return_tensors=\"pt\").input_values","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:28.033362Z","iopub.execute_input":"2025-07-22T08:36:28.034041Z","iopub.status.idle":"2025-07-22T08:36:28.043582Z","shell.execute_reply.started":"2025-07-22T08:36:28.034018Z","shell.execute_reply":"2025-07-22T08:36:28.042764Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"with torch.no_grad():\n    logits = model(input_values).logits","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:28.044483Z","iopub.execute_input":"2025-07-22T08:36:28.044792Z","iopub.status.idle":"2025-07-22T08:36:30.162451Z","shell.execute_reply.started":"2025-07-22T08:36:28.044756Z","shell.execute_reply":"2025-07-22T08:36:30.161549Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Take argmax to get the most probable phoneme IDs\npredicted_ids = torch.argmax(logits, dim=-1)\n\n# Decode the predicted IDs to phoneme string\nphoneme_transcription = processor.batch_decode(predicted_ids)\n\nprint(f\"Phoneme Transcription: {phoneme_transcription}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:30.163376Z","iopub.execute_input":"2025-07-22T08:36:30.163677Z","iopub.status.idle":"2025-07-22T08:36:30.169625Z","shell.execute_reply.started":"2025-07-22T08:36:30.163646Z","shell.execute_reply":"2025-07-22T08:36:30.168891Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"phoneme = \" \".join(phoneme_transcription)\nphoneme","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:30.170490Z","iopub.execute_input":"2025-07-22T08:36:30.170764Z","iopub.status.idle":"2025-07-22T08:36:30.183326Z","shell.execute_reply.started":"2025-07-22T08:36:30.170739Z","shell.execute_reply":"2025-07-22T08:36:30.182395Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torchaudio\nfrom transformers import AutoTokenizer, AutoModelForSpeechSeq2Seq, AutoProcessor, BitsAndBytesConfig\nimport librosa\nimport numpy as np\nfrom tqdm import tqdm\nfrom transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\nimport torch","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:30.184268Z","iopub.execute_input":"2025-07-22T08:36:30.184590Z","iopub.status.idle":"2025-07-22T08:36:30.489634Z","shell.execute_reply.started":"2025-07-22T08:36:30.184569Z","shell.execute_reply":"2025-07-22T08:36:30.488956Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel_id = \"openai/whisper-small.en\"\n\n# Load processor\nprocessor = AutoProcessor.from_pretrained(model_id)\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n\n\nmodel = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\nmodel.to(device)\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:30.490400Z","iopub.execute_input":"2025-07-22T08:36:30.490622Z","iopub.status.idle":"2025-07-22T08:36:39.420906Z","shell.execute_reply.started":"2025-07-22T08:36:30.490605Z","shell.execute_reply":"2025-07-22T08:36:39.420081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Alternative approach using Transformers pipeline (more robust)\nfrom transformers import pipeline\n\ndef audio_to_text_pipeline(audio_file_path, chunk_length_s=30):\n    \"\"\"\n    Convert audio file to text using Transformers pipeline (most reliable method)\n\n    Args:\n        audio_file_path (str): Path to the audio file\n        chunk_length_s (int): Length of audio chunks in seconds\n\n    Returns:\n        str: Transcribed text\n    \"\"\"\n    try:\n        # Create a speech recognition pipeline\n        pipe = pipeline(\n            \"automatic-speech-recognition\",\n            model=model_id,\n            device=0 if torch.cuda.is_available() else -1,\n            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n            chunk_length_s=chunk_length_s,\n            return_timestamps=True  # Enable timestamps for better alignment\n        )\n\n        # Process the audio file\n        result = pipe(audio_file_path)\n\n        # Extract text from result\n        if isinstance(result, dict):\n            return result.get(\"text\", \"\")\n        elif isinstance(result, list):\n            return \" \".join([chunk.get(\"text\", \"\") for chunk in result])\n        else:\n            return str(result)\n\n    except Exception as e:\n        print(f\"Pipeline method failed: {e}\")\n        print(\"Falling back to manual method...\")\n        return audio_to_text_chunked(audio_file_path, chunk_length_s=chunk_length_s)\n\ndef audio_to_text_simple(audio_file_path):\n    \"\"\"\n    Simple audio to text conversion using librosa and basic processing\n\n    Args:\n        audio_file_path (str): Path to the audio file\n\n    Returns:\n        str: Transcribed text\n    \"\"\"\n    try:\n        # Try pipeline method first (most reliable)\n        return audio_to_text_pipeline(audio_file_path)\n    except Exception as e:\n        print(f\"Pipeline failed, trying chunked method: {e}\")\n        try:\n            # Fallback to chunked method\n            return audio_to_text_chunked(audio_file_path)\n        except Exception as e2:\n            print(f\"Chunked method failed: {e2}\")\n            # Final fallback\n            return audio_to_text(audio_file_path)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:39.421852Z","iopub.execute_input":"2025-07-22T08:36:39.422146Z","iopub.status.idle":"2025-07-22T08:36:39.521962Z","shell.execute_reply.started":"2025-07-22T08:36:39.422118Z","shell.execute_reply":"2025-07-22T08:36:39.521355Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"transcript = audio_to_text_simple(\"/kaggle/input/audiotest/Recording.wav\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:39.522804Z","iopub.execute_input":"2025-07-22T08:36:39.523026Z","iopub.status.idle":"2025-07-22T08:36:44.484786Z","shell.execute_reply.started":"2025-07-22T08:36:39.523008Z","shell.execute_reply":"2025-07-22T08:36:44.484044Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(transcript)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:44.486074Z","iopub.execute_input":"2025-07-22T08:36:44.486418Z","iopub.status.idle":"2025-07-22T08:36:44.491331Z","shell.execute_reply.started":"2025-07-22T08:36:44.486390Z","shell.execute_reply":"2025-07-22T08:36:44.490382Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import phonemizer\nfrom phonemizer.punctuation import Punctuation\nfrom phonemizer.backend import EspeakBackend\nfrom phonemizer.separator import Separator\n\ndef generate_reference_phoneme(reference_text):\n    text = Punctuation(';:,.!\"?()').remove(reference_text)\n    ref_words = [w.lower() for w in text.strip().split(' ') if w]\n    \n    \n    # initialize the espeak backend for English\n    backend = EspeakBackend('en-us')\n    \n    # separate phones by a space and ignoring words boundaries\n    separator = Separator(phone='', word=None)\n    \n    # build the lexicon by phonemizing each word one by one. The backend.phonemize\n    # function expect a list as input and outputs a list.\n    lexicon = [ (word, backend.phonemize([word], separator=separator, strip=True)[0])\n        for word in ref_words]\n    \n    return lexicon, ref_words ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:44.492390Z","iopub.execute_input":"2025-07-22T08:36:44.492671Z","iopub.status.idle":"2025-07-22T08:36:44.507639Z","shell.execute_reply.started":"2025-07-22T08:36:44.492653Z","shell.execute_reply":"2025-07-22T08:36:44.506534Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"lexicon, ref_words = generate_reference_phoneme(transcript)\nreference_phoneme =' '.join([phon for w, phon in lexicon])\nreference_phoneme","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:44.508784Z","iopub.execute_input":"2025-07-22T08:36:44.509123Z","iopub.status.idle":"2025-07-22T08:36:44.578387Z","shell.execute_reply.started":"2025-07-22T08:36:44.509095Z","shell.execute_reply":"2025-07-22T08:36:44.577590Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sequence_align.pairwise import hirschberg, needleman_wunsch\nseq_a = reference_phoneme\nseq_b = list(phoneme.replace(' ',''))\n\n# recorded_phoneme['text']\naligned_seq_a, aligned_seq_b = needleman_wunsch(\n    seq_a,\n    seq_b,\n    match_score=1.0,\n    mismatch_score=-1.0,\n    indel_score=-1.0,\n    gap=\"_\",\n)\naligned_reference_seq = ''.join(aligned_seq_a)\naligned_recorded_seq = ''.join(aligned_seq_b)\n\nprint('Reference Text: ', transcript)\nprint('Reference Phoneme:',aligned_reference_seq)\nprint('Recorded Phoneme: ', aligned_recorded_seq)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:44.579377Z","iopub.execute_input":"2025-07-22T08:36:44.579687Z","iopub.status.idle":"2025-07-22T08:36:44.593991Z","shell.execute_reply.started":"2025-07-22T08:36:44.579658Z","shell.execute_reply":"2025-07-22T08:36:44.593244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import re\n\ndef find_word_start_positions(reference_sequence):\n    # Split the sequence into words based on spaces\n    words = reference_sequence.split()\n    # Initialize a list to store the start positions\n    start_positions = []\n    # Initialize the current position\n    current_position = 0\n    # Iterate over the words\n    for word in words:\n        # Add the current position to the start positions list\n        start_positions.append(current_position)\n        # Increment the current position by the length of the word plus 1 (for the space)\n        current_position += len(word) + 1\n    return start_positions\n\ndef split_recorded_sequence(recorded_sequence, start_positions):\n    # Initialize a list to store the split words\n    split_words = []\n    # Iterate over the start positions\n    for i in range(len(start_positions)):\n        # Get the start position\n        start = start_positions[i]\n        # If it's the last word, get the end position as the length of the sequence\n        if i == len(start_positions) - 1:\n            end = len(recorded_sequence)\n        # Otherwise, get the end position as the start position of the next word\n        else:\n            end = start_positions[i + 1]\n        # Extract the word from the recorded sequence\n        word = recorded_sequence[start:end]\n        # Add the word to the list\n        split_words.append(word)\n    return split_words\n    \n# recorded_sequence = \"aɪ_hoːp_ðeɪ_hɛv_maɪ_fiːv__rədbrænd_aɪl_biː_bæk_su_n__tʊ_pliːz_w_iːdfoː__miː_\"\nref_start_positions = find_word_start_positions(''.join(aligned_reference_seq))\n\n# split recorded based on the reference start positions\nrec_split_words = split_recorded_sequence(''.join(aligned_recorded_seq), ref_start_positions)\nrec_split_words = [re.sub('( |\\\\_)$','',w) for w in rec_split_words]\n\n# split ref based on the reference start positions\nref_split_words = split_recorded_sequence(''.join(aligned_reference_seq), ref_start_positions)\nref_split_words = [re.sub('(\\\\_| )$','',w) for w in ref_split_words]\n\n# print('Reference Text: ',reference_text)\n# print('(word, reference_phoneme, recorded_phoneme)',list(zip(ref_words, ref_split_words, rec_split_words)))\nword_comparision_list = list(zip(ref_words, ref_split_words, rec_split_words))\nword_comparision_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:44.594839Z","iopub.execute_input":"2025-07-22T08:36:44.595041Z","iopub.status.idle":"2025-07-22T08:36:44.606338Z","shell.execute_reply.started":"2025-07-22T08:36:44.595025Z","shell.execute_reply":"2025-07-22T08:36:44.605487Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\nmax_length = max(len(w) for w, _, _ in word_comparision_list)\n\nfor w, ref_w, rec_w in word_comparision_list:\n    word = f\"\\033[1m{w}\\033[0m\".ljust(max_length)\n    if ref_w == rec_w:\n        rec_string = f\"\\033[92m{rec_w}\\033[0m\".ljust(max_length)  # Green color\n    else:\n        mismatch_index = 0\n        for i, (c1, c2) in enumerate(zip(ref_w, rec_w)):\n            if c1 != c2:\n                mismatch_index = i\n                break\n        rec_string = \"{}\\033[91m{}\\033[0m{}\".format(rec_w[:mismatch_index], rec_w[mismatch_index], rec_w[mismatch_index+1:]).ljust(max_length)\n\n    print(word, ref_w, rec_string)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:44.607344Z","iopub.execute_input":"2025-07-22T08:36:44.607605Z","iopub.status.idle":"2025-07-22T08:36:44.623693Z","shell.execute_reply.started":"2025-07-22T08:36:44.607585Z","shell.execute_reply":"2025-07-22T08:36:44.622611Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%capture\nimport os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n!pip install unsloth\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:44.628326Z","iopub.execute_input":"2025-07-22T08:36:44.628618Z","iopub.status.idle":"2025-07-22T08:36:48.618650Z","shell.execute_reply.started":"2025-07-22T08:36:44.628598Z","shell.execute_reply":"2025-07-22T08:36:48.617404Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\n\nfourbit_models = [\n    \"unsloth/Qwen3-1.7B-unsloth-bnb-4bit\", # Qwen 14B 2x faster\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-14B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-32B-unsloth-bnb-4bit\",\n\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/Phi-4\",\n    \"unsloth/Llama-3.1-8B\",\n    \"unsloth/Llama-3.2-3B\",\n    \"unsloth/orpheus-3b-0.1-ft-unsloth-bnb-4bit\" # [NEW] We support TTS models!\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"Qwen/Qwen3-1.7B\",\n    max_seq_length = 10000,   # Context length - can be longer, but uses more memory\n    load_in_4bit = False,     # 4bit uses much less memory\n    load_in_8bit = False,    # A bit more accurate, uses 2x memory\n    full_finetuning = False, # We have full finetuning now!\n    # token = \"hf_...\",      # use one if using gated models\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:36:48.620149Z","iopub.execute_input":"2025-07-22T08:36:48.620893Z","iopub.status.idle":"2025-07-22T08:37:07.993141Z","shell.execute_reply.started":"2025-07-22T08:36:48.620852Z","shell.execute_reply":"2025-07-22T08:37:07.992188Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"irubic = {\"Pronunciation\": [\"Band 1 : Can produce occasional individual words and phonemes that are recognisable, but no overall meaning is conveyed. Unintelligible.\",\"Band 2 : Uses few acceptable phonological features (possibly because sample is insufficient). Overall problems with delivery impair attempts at connected speech. Individual words and phonemes are mainly mispronounced and little meaning is conveyed. Often unintelligible.\"\n,\"Band 3 : Displays some features of band 2, and some, but not all, of the positive features of band 4.\", \"Band 4 : Uses some acceptable phonological features, but the range is limited. Produces some acceptable chunking, but there are frequent lapses in overall rhythm. Attempts to use intonation and stress, but control is limited. Individual words or phonemes are frequently mispronounced, causing lack of clarity. Understanding requires some effort and there may be patches of speech that cannot be understood.\",\n\"Band 5 : Displays all the positive features of band 4, and some, but not all, of the positive features of band 6.\", \"Band 6 : Uses a range of phonological features, but control is variable. Chunking is generally appropriate, but rhythm may be affected by a lack of stress-timing and/or a rapid speech rate. Some effective use of intonation and stress, but this is not sustained. Individual words or phonemes may be mispronounced but this causes only occasional lack of clarity. Can generally be understood throughout without much effort.\",\n\"Band 7 : Displays all the positive features of band 6, and some, but not all, of the positive features of band 8.\", \"Band 8 : Uses a wide range of phonological features to convey precise and/or subtle meaning. Can sustain appropriate rhythm. Flexible use of stress and intonation across long utterances, despite occasional lapses. Can be easily understood throughout. Accent has minimal effect on intelligibility.\", \"Band 9 : Uses a full range of phonological features to convey precise and/or subtle meaning. Flexible use of features of connected speech is sustained throughout. Can be effortlessly understood throughout. Accent has no effect on intelligibility.\"\n    ], \"Grammatical range and accuracy\" : [\n    \"Band 1 : No rateable languageunless memorised.\",\n    \"Band 2 : No evidence of basic sentence forms.\",\n    \"Band 3 : Basic sentence forms are attempted but grammatical errors are numerous except in apparently memorised utterances.\",\n    \"Band 4 : Can produce basic sentence forms and some short utterances are error-free. Subordinate clauses are rare and, overall, turns are short, structures are repetitive and errors are frequent.\",\n    \"Band 5 : Basic sentence forms are fairly well controlled for accuracy. Complex structures are attempted but these are limited in range, nearly always contain errors and may lead to the need for reformulation.\",\n    \"Band 6 : Produces a mix of short and complex sentence forms and a variety of structures with limited flexibility. Though errors frequently occur in complex structures, these rarely impede communication.\",\n    \"Band 7 : A range of structures flexibly used. Error-free sentences are frequent. Both simple and complex sentences are used effectively despite some errors. A few basic errors persist.\",\n    \"Band 8 : Wide range of structures, flexibly used. The majority of sentences are error free. Occasional inappropriaciesand non-systematic errors occur. A few basic errors may persist.\",\n    \"Band 9 : Structures are precise and accurate at all times, apart from ‘mistakes’ characteristic of native speaker speech.\"\n    ], \"Lexical resource\" : [\n    \"Band 1 : No resource bar a few isolated words. No communication possible.\",\n    \"Band 2 : Very limited resource. Utterances consist of isolated words or memorised utterances. Little communication possible without the support of mime or gesture.\",\n    \"Band 3 : Resource limited to simple vocabulary used primarily to convey personal information. Vocabulary inadequate for unfamiliar topics.\",\n    \"Band 4 : Resource sufficient for familiar topics but only basic meaning can be conveyed on unfamiliar topics. Frequent inappropriaciesand errors in word choice. Rarely attempts paraphrase.\",\n    \"Band 5 : Resource sufficient to discuss familiar and unfamiliar topics but there is limited flexibility. Attempts paraphrase but not always with success.\",\n    \"Band 6 : Resource sufficient to discuss topics at length. Vocabulary use may be inappropriate but meaning is clear. Generally able to paraphrase successfully.\",\n    \"Band 7 : Resource flexibly used to discuss a variety of topics. Some ability to use less common and idiomatic items and an awareness of style and collocation is evident though inappropriaciesoccur. Effective use of paraphrase as required.\",\n    \"Band 8 : Wide resource, readily and flexibly used to discuss all topics and convey precise meaning. Skilful use of less common and idiomatic items despite occasional inaccuracies in word choice and collocation. Effective use of paraphrase as required.\",\n    \"Band 9 : Total flexibility and precise use in all contexts. Sustained use of accurate and idiomatic language.\"\n    ]}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:37:07.994525Z","iopub.execute_input":"2025-07-22T08:37:07.994776Z","iopub.status.idle":"2025-07-22T08:37:08.000780Z","shell.execute_reply.started":"2025-07-22T08:37:07.994757Z","shell.execute_reply":"2025-07-22T08:37:07.999863Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Automated Qwen 3 Pronunciation Error Analysis for Each Word\nimport re\nfrom transformers import TextStreamer\n\n\ndef analyze_pronunciation_with_qwen( transcript, alignments,rubic, model, tokenizer, max_new_tokens=32):\n  \"\"\"\n  For each word, send a prompt to Qwen 3 with audio, transcript, expected and actual phonemes.\n  Output: If a word has a phoneme error, output only the word and the single incorrect phoneme (no explanation). If the word is correct, output only '1'.\n  \"\"\"\n  prompt = f\"\"\"\n  You are a strict speaking evaluator for the IELTS exam. Given the following information:\n  - Transcript: {transcript}\n  - Phonemes : (word, expected phoneme, observed phoneme) : {word_comparision_list}\n  give feedback on the pronounciation, Grammatical range and accuracy, and Lexical resource of the speaker, for each of the 3 cretiria output only a sentece of feedback, your output should be in the form of :\n  Pronounciation : feedback (i.e you have a problem pronouncing X like in Y)\n  Grammatical range and accuracy : feedback (do not mention anything about pronounciation here)\n  Lexical resource of the speaker : feedback (do not mention anything about pronounciation here)\n  Follow the IELTS speaking descriptors to give the speaker a very accurate feedback\n  Pronounciation rubic is {rubic[\"Pronunciation\"]}\n  Grammatical range and accuracy is {rubic[\"Grammatical range and accuracy\"]}\n  Lexical resource is {rubic[\"Lexical resource\"]}\n         \"\"\"\n  messages = [{\n      \"role\": \"user\",\n      \"content\": prompt\n  }]\n  text = tokenizer.apply_chat_template(\n      messages,\n      tokenize=False,\n      add_generation_prompt=True,\n      enable_thinking=False,\n  )\n  print(f\"\\n---\\nAnalyzing words:\")\n  _ = model.generate(\n      **tokenizer(text, return_tensors=\"pt\").to(\"cuda\"),\n      max_new_tokens=max_new_tokens,\n      temperature=0.7, top_p=0.8, top_k=20,\n      streamer=TextStreamer(tokenizer, skip_prompt=True),\n  )\n# Example usage (replace with your actual variables):\nanalyze_pronunciation_with_qwen(\n  transcript=transcript,\n  alignments=word_comparision_list,\n  rubic=irubic,\n  model=model,\n  tokenizer=tokenizer,\n  max_new_tokens=10000 )","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:37:08.001785Z","iopub.execute_input":"2025-07-22T08:37:08.002075Z","iopub.status.idle":"2025-07-22T08:37:17.110222Z","shell.execute_reply.started":"2025-07-22T08:37:08.002055Z","shell.execute_reply":"2025-07-22T08:37:17.109418Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#torch.cuda.empty_cache()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:37:17.111162Z","iopub.execute_input":"2025-07-22T08:37:17.111436Z","iopub.status.idle":"2025-07-22T08:37:17.115128Z","shell.execute_reply.started":"2025-07-22T08:37:17.111419Z","shell.execute_reply":"2025-07-22T08:37:17.114296Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#del model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:37:17.116168Z","iopub.execute_input":"2025-07-22T08:37:17.116425Z","iopub.status.idle":"2025-07-22T08:37:17.130115Z","shell.execute_reply.started":"2025-07-22T08:37:17.116406Z","shell.execute_reply":"2025-07-22T08:37:17.129263Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!sudo apt-get install portaudio19-dev python3-dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:38:54.393118Z","iopub.execute_input":"2025-07-22T08:38:54.393491Z","iopub.status.idle":"2025-07-22T08:39:02.526435Z","shell.execute_reply.started":"2025-07-22T08:38:54.393458Z","shell.execute_reply":"2025-07-22T08:39:02.525527Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install pyaudio\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T08:39:13.937838Z","iopub.execute_input":"2025-07-22T08:39:13.938540Z","iopub.status.idle":"2025-07-22T08:39:24.668859Z","shell.execute_reply.started":"2025-07-22T08:39:13.938507Z","shell.execute_reply":"2025-07-22T08:39:24.667618Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}