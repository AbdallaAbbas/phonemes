{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "147e35ab",
   "metadata": {},
   "source": [
    "# This is the speaking part of GenIELTS, a phonemes error detection system"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f11a72a",
   "metadata": {},
   "source": [
    "### First we should convert audio input to a text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "13d1838f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from transformers import AutoTokenizer, AutoModelForSpeechSeq2Seq, AutoProcessor, BitsAndBytesConfig\n",
    "import librosa\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2ForCTC\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acfca73b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WhisperForConditionalGeneration(\n",
       "  (model): WhisperModel(\n",
       "    (encoder): WhisperEncoder(\n",
       "      (conv1): Conv1d(80, 768, kernel_size=(3,), stride=(1,), padding=(1,))\n",
       "      (conv2): Conv1d(768, 768, kernel_size=(3,), stride=(2,), padding=(1,))\n",
       "      (embed_positions): Embedding(1500, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperEncoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (activation_fn): GELUActivation()\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "    (decoder): WhisperDecoder(\n",
       "      (embed_tokens): Embedding(51864, 768, padding_idx=50256)\n",
       "      (embed_positions): WhisperPositionalEmbedding(448, 768)\n",
       "      (layers): ModuleList(\n",
       "        (0-11): 12 x WhisperDecoderLayer(\n",
       "          (self_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (activation_fn): GELUActivation()\n",
       "          (self_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (encoder_attn): WhisperAttention(\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "          (encoder_attn_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "          (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (final_layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "    )\n",
       "  )\n",
       "  (proj_out): Linear(in_features=768, out_features=51864, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_id = \"openai/whisper-small.en\"\n",
    "\n",
    "# Load processor\n",
    "processor = AutoProcessor.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "\n",
    "\n",
    "model = AutoModelForSpeechSeq2Seq.from_pretrained(model_id)\n",
    "model.to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "868a5b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def audio_to_text(audio_file_path, sample_rate=16000):\n",
    "    \"\"\"\n",
    "    Convert audio file to text using Whisper model correctly\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file\n",
    "        sample_rate (int): Target sample rate for the model\n",
    "    \n",
    "    Returns:\n",
    "        str: Transcribed text\n",
    "    \"\"\"\n",
    "    # Load and preprocess audio\n",
    "    audio, sr = librosa.load(audio_file_path, sr=sample_rate)\n",
    "    \n",
    "    # Process audio with the model's processor\n",
    "    inputs = processor(audio, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "    \n",
    "    # Move inputs to device\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    \n",
    "    # Generate transcription using the proper generation method\n",
    "    with torch.no_grad():\n",
    "        # Use model.generate() instead of direct forward pass\n",
    "        generated_ids = model.generate(\n",
    "            inputs[\"input_features\"],\n",
    "            max_length=448,\n",
    "            do_sample=False,\n",
    "            temperature=0.0,\n",
    "            return_timestamps=False\n",
    "        )\n",
    "    \n",
    "    # Decode the generated IDs to text\n",
    "    transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    \n",
    "    return transcription\n",
    "\n",
    "def audio_to_text_chunked(audio_file_path, sample_rate=16000, chunk_length_s=30):\n",
    "    \"\"\"\n",
    "    Convert audio file to text using Whisper model with chunking.\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file\n",
    "        sample_rate (int): Target sample rate for the model\n",
    "        chunk_length_s (int): Length of audio chunks in seconds\n",
    "    \n",
    "    Returns:\n",
    "        str: Transcribed text\n",
    "    \"\"\"\n",
    "    # Load audio\n",
    "    audio, sr = librosa.load(audio_file_path, sr=sample_rate)\n",
    "    \n",
    "    chunk_size = chunk_length_s * sample_rate\n",
    "    num_chunks = (len(audio) + chunk_size - 1) // chunk_size\n",
    "    \n",
    "    full_transcription = \"\"\n",
    "    \n",
    "    for i in tqdm(range(num_chunks), desc=f\"Chunking {audio_file_path}\"):\n",
    "        start = i * chunk_size\n",
    "        end = start + chunk_size\n",
    "        chunk = audio[start:end]\n",
    "        \n",
    "        # Process audio with the model's processor\n",
    "        inputs = processor(chunk, sampling_rate=sample_rate, return_tensors=\"pt\")\n",
    "\n",
    "        # Move inputs to device\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate transcription using the proper generation method\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(\n",
    "                inputs[\"input_features\"],\n",
    "                max_length=448,\n",
    "                do_sample=False,\n",
    "                temperature=0.0,\n",
    "                return_timestamps=False\n",
    "            )\n",
    "\n",
    "        # Decode the generated IDs to text\n",
    "        transcription = processor.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        full_transcription += transcription + \" \"\n",
    "        \n",
    "    return full_transcription.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d529ada0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative approach using Transformers pipeline (more robust)\n",
    "from transformers import pipeline\n",
    "\n",
    "def audio_to_text_pipeline(audio_file_path, chunk_length_s=30):\n",
    "    \"\"\"\n",
    "    Convert audio file to text using Transformers pipeline (most reliable method)\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file\n",
    "        chunk_length_s (int): Length of audio chunks in seconds\n",
    "    \n",
    "    Returns:\n",
    "        str: Transcribed text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Create a speech recognition pipeline\n",
    "        pipe = pipeline(\n",
    "            \"automatic-speech-recognition\",\n",
    "            model=model_id,\n",
    "            device=0 if torch.cuda.is_available() else -1,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            chunk_length_s=chunk_length_s,\n",
    "            return_timestamps=True  # Enable timestamps for better alignment\n",
    "        )\n",
    "        \n",
    "        # Process the audio file\n",
    "        result = pipe(audio_file_path)\n",
    "        \n",
    "        # Extract text from result\n",
    "        if isinstance(result, dict):\n",
    "            return result.get(\"text\", \"\")\n",
    "        elif isinstance(result, list):\n",
    "            return \" \".join([chunk.get(\"text\", \"\") for chunk in result])\n",
    "        else:\n",
    "            return str(result)\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline method failed: {e}\")\n",
    "        print(\"Falling back to manual method...\")\n",
    "        return audio_to_text_chunked(audio_file_path, chunk_length_s=chunk_length_s)\n",
    "\n",
    "def audio_to_text_simple(audio_file_path):\n",
    "    \"\"\"\n",
    "    Simple audio to text conversion using librosa and basic processing\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file\n",
    "    \n",
    "    Returns:\n",
    "        str: Transcribed text\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Try pipeline method first (most reliable)\n",
    "        return audio_to_text_pipeline(audio_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Pipeline failed, trying chunked method: {e}\")\n",
    "        try:\n",
    "            # Fallback to chunked method\n",
    "            return audio_to_text_chunked(audio_file_path)\n",
    "        except Exception as e2:\n",
    "            print(f\"Chunked method failed: {e2}\")\n",
    "            # Final fallback\n",
    "            return audio_to_text(audio_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "72bbfbd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Audio Transcription Methods\n",
      "==================================================\n",
      "Testing with: Recording.wav\n",
      "\n",
      "1. Testing audio_to_text_simple()...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Using `chunk_length_s` is very experimental with seq2seq models. The results will not necessarily be entirely accurate and will have caveats. More information: https://github.com/huggingface/transformers/pull/20104. Ignore this warning with pipeline(..., ignore_warning=True). To use Whisper for long-form transcription, use rather the model's `generate` method directly as the model relies on it's own chunking mechanism (cf. Whisper original paper, section 3.8. Long-form Transcription).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline method failed: ffmpeg was not found but is required to load audio files from filename\n",
      "Falling back to manual method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Chunking Recording.wav:   0%|          | 0/1 [00:00<?, ?it/s]Using custom `forced_decoder_ids` from the (generation) config. This is deprecated in favor of the `task` and `language` flags/config options.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Chunking Recording.wav: 100%|██████████| 1/1 [00:01<00:00,  1.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Success! Transcript: 'How are you? What are you doing? I would like...'\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Test audio transcription functionality\n",
    "print(\"Testing Audio Transcription Methods\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Check if audio file exists\n",
    "import os\n",
    "audio_files = [\"Recording.wav\", \"test.wav\", \"artest.wav\"]\n",
    "available_files = [f for f in audio_files if os.path.exists(f)]\n",
    "\n",
    "if available_files:\n",
    "    test_file = available_files[0]\n",
    "    print(f\"Testing with: {test_file}\")\n",
    "    \n",
    "    try:\n",
    "        # Test the simple method\n",
    "        print(\"\\n1. Testing audio_to_text_simple()...\")\n",
    "        transcript = audio_to_text_simple(test_file)\n",
    "        print(f\"✓ Success! Transcript: '{transcript[:100]}...'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error in audio_to_text_simple: {e}\")\n",
    "        \n",
    "        try:\n",
    "            # Test pipeline method directly\n",
    "            print(\"\\n2. Testing pipeline method...\")\n",
    "            transcript = audio_to_text_pipeline(test_file)\n",
    "            print(f\"✓ Pipeline success! Transcript: '{transcript[:100]}...'\")\n",
    "            \n",
    "        except Exception as e2:\n",
    "            print(f\"❌ Pipeline error: {e2}\")\n",
    "            \n",
    "            try:\n",
    "                # Test basic method\n",
    "                print(\"\\n3. Testing basic method...\")\n",
    "                transcript = audio_to_text(test_file)\n",
    "                print(f\"✓ Basic method success! Transcript: '{transcript[:100]}...'\")\n",
    "                \n",
    "            except Exception as e3:\n",
    "                print(f\"❌ All methods failed: {e3}\")\n",
    "                print(\"Please check your model and audio file setup.\")\n",
    "                \n",
    "else:\n",
    "    print(\"❌ No audio files found for testing.\")\n",
    "    print(\"Available files in directory:\", os.listdir(\".\"))\n",
    "\n",
    "print(\"=\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ad4a7338",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def parse_britfone(file_path):\n",
    "    \"\"\"\n",
    "    Parses the Britfone csv file into a dictionary.\n",
    "    \"\"\"\n",
    "    pronunciations = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        for row in reader:\n",
    "            word = row[0].lower()\n",
    "            phonemes = row[1]\n",
    "            \n",
    "            # Handle multiple pronunciations (e.g., word(1))\n",
    "            word = re.sub(r'\\(\\d+\\)', '', word)\n",
    "            \n",
    "            if word in pronunciations:\n",
    "                if phonemes not in pronunciations[word]:\n",
    "                    pronunciations[word].append(phonemes)\n",
    "            else:\n",
    "                pronunciations[word] = [phonemes]\n",
    "    return pronunciations\n",
    "\n",
    "# Parse the downloaded Britfone file\n",
    "britfone_lexicon = parse_britfone(\"britfone.main.3.0.1.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65ce1fed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from g2p_en import G2p\n",
    "\n",
    "# Initialize the G2P converter\n",
    "g2p = G2p()\n",
    "\n",
    "def alternative_g2p(word):\n",
    "    \"\"\"\n",
    "    Alternative G2P using g2p-en library (doesn't require espeak).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        phonemes = g2p(word)\n",
    "        # Convert to a format similar to britfone\n",
    "        return ' '.join(phonemes)\n",
    "    except Exception as e:\n",
    "        print(f\"G2P failed for '{word}': {e}\")\n",
    "        return word.lower()\n",
    "\n",
    "def get_phonemes_enhanced(word, lexicon):\n",
    "    \"\"\"\n",
    "    Enhanced phoneme lookup with multiple fallback options.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    if word in lexicon:\n",
    "        return lexicon[word][0]  # Return the first pronunciation from Britfone\n",
    "    else:\n",
    "        # Try alternative G2P first\n",
    "        try:\n",
    "            return alternative_g2p(word)\n",
    "        except Exception as e:\n",
    "            print(f\"Alternative G2P failed for '{word}': {e}\")\n",
    "            # Final fallback - simple phonetic approximation\n",
    "            return word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3856350d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcript_to_phonemes(transcript, lexicon):\n",
    "    \"\"\"\n",
    "    Convert transcript text to phonemes and print the results.\n",
    "    \n",
    "    Args:\n",
    "        transcript (str): The transcribed text from audio\n",
    "        lexicon (dict): The phoneme dictionary (britfone_lexicon)\n",
    "    \"\"\"\n",
    "    # Clean and split the transcript into words\n",
    "    words = re.sub(r'[^\\w\\s]', '', transcript.lower()).split()\n",
    "    \n",
    "    print(f\"Original transcript: {transcript}\")\n",
    "    print(f\"Number of words: {len(words)}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    phoneme_results = []\n",
    "    \n",
    "    for i, word in enumerate(words, 1):\n",
    "        phonemes = get_phonemes_enhanced(word, lexicon)\n",
    "        phoneme_results.append((word, phonemes))\n",
    "        print(f\"{i:2d}. Word: '{word}' -> Phonemes: [{phonemes}]\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"Complete phoneme sequence:\")\n",
    "    print(\" | \".join([phonemes for word, phonemes in phoneme_results]))\n",
    "    \n",
    "    return phoneme_results\n",
    "\n",
    "# Example usage with audio transcription\n",
    "def process_audio_to_phonemes(audio_file_path):\n",
    "    \"\"\"\n",
    "    Complete pipeline: Audio -> Transcript -> Phonemes\n",
    "    \n",
    "    Args:\n",
    "        audio_file_path (str): Path to the audio file\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (word, phonemes) tuples\n",
    "    \"\"\"\n",
    "    print(f\"Processing audio file: {audio_file_path}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Step 1: Convert audio to text using the most reliable method\n",
    "    try:\n",
    "        transcript = audio_to_text_simple(audio_file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"All audio transcription methods failed: {e}\")\n",
    "        print(\"Please check your audio file and model setup.\")\n",
    "        return \"\", []\n",
    "    \n",
    "    # Step 2: Convert transcript to phonemes\n",
    "    phoneme_results = transcript_to_phonemes(transcript, britfone_lexicon)\n",
    "    \n",
    "    return transcript, phoneme_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3aa9a774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IPA to ARPABET and ARPABET to IPA mapping dictionaries\n",
    "# Based on CMU Pronouncing Dictionary and standard phonetic mappings\n",
    "\n",
    "IPA_TO_ARPABET = {\n",
    "    # Vowels\n",
    "    'i': 'IY',      # beat\n",
    "    'ɪ': 'IH',      # bit\n",
    "    'e': 'EY',      # bait\n",
    "    'ɛ': 'EH',      # bet\n",
    "    'æ': 'AE',      # bat\n",
    "    'ɑ': 'AA',      # bot\n",
    "    'ɔ': 'AO',      # bought\n",
    "    'o': 'OW',      # boat\n",
    "    'ʊ': 'UH',      # book\n",
    "    'u': 'UW',      # boot\n",
    "    'ʌ': 'AH',      # but\n",
    "    'ə': 'AH',      # about (schwa -> AH)\n",
    "    'ɚ': 'ER',      # butter\n",
    "    'ɝ': 'ER',      # bird\n",
    "    \n",
    "    # Diphthongs\n",
    "    'aɪ': 'AY',     # bite\n",
    "    'aʊ': 'AW',     # bout\n",
    "    'ɔɪ': 'OY',     # boy\n",
    "    'eɪ': 'EY',     # bait\n",
    "    'oʊ': 'OW',     # boat\n",
    "    'ɪə': 'IH R',   # beer\n",
    "    'ɛə': 'EH R',   # bear\n",
    "    'ʊə': 'UH R',   # tour\n",
    "    \n",
    "    # Consonants\n",
    "    'p': 'P',       # pat\n",
    "    'b': 'B',       # bat\n",
    "    't': 'T',       # tat\n",
    "    'd': 'D',       # dad\n",
    "    'k': 'K',       # cat\n",
    "    'g': 'G',       # gap\n",
    "    'f': 'F',       # fat\n",
    "    'v': 'V',       # vat\n",
    "    'θ': 'TH',      # think\n",
    "    'ð': 'DH',      # that\n",
    "    's': 'S',       # sat\n",
    "    'z': 'Z',       # zap\n",
    "    'ʃ': 'SH',      # ship\n",
    "    'ʒ': 'ZH',      # measure\n",
    "    'h': 'HH',      # hat\n",
    "    'm': 'M',       # mat\n",
    "    'n': 'N',       # nat\n",
    "    'ŋ': 'NG',      # sing\n",
    "    'l': 'L',       # lat\n",
    "    'r': 'R',       # rat\n",
    "    'w': 'W',       # way\n",
    "    'j': 'Y',       # yet\n",
    "    \n",
    "    # Affricates\n",
    "    'tʃ': 'CH',     # church\n",
    "    'dʒ': 'JH',     # judge\n",
    "    \n",
    "    # Additional symbols\n",
    "    'ʔ': '',        # glottal stop (often omitted in ARPABET)\n",
    "    ' ': ' ',       # word boundary\n",
    "    '.': '',        # syllable boundary (omitted in ARPABET)\n",
    "}\n",
    "\n",
    "# Create reverse mapping\n",
    "ARPABET_TO_IPA = {v: k for k, v in IPA_TO_ARPABET.items() if v != ''}\n",
    "\n",
    "# Handle special cases for reverse mapping\n",
    "ARPABET_TO_IPA.update({\n",
    "    'IY': 'i',\n",
    "    'IH': 'ɪ', \n",
    "    'EY': 'eɪ',\n",
    "    'EH': 'ɛ',\n",
    "    'AE': 'æ',\n",
    "    'AA': 'ɑ',\n",
    "    'AO': 'ɔ',\n",
    "    'OW': 'oʊ',\n",
    "    'UH': 'ʊ',\n",
    "    'UW': 'u',\n",
    "    'AH': 'ʌ',  # Primary mapping for AH\n",
    "    'ER': 'ɝ',\n",
    "    'AY': 'aɪ',\n",
    "    'AW': 'aʊ',\n",
    "    'OY': 'ɔɪ',\n",
    "    'HH': 'h',\n",
    "    'CH': 'tʃ',\n",
    "    'JH': 'dʒ',\n",
    "    'TH': 'θ',\n",
    "    'DH': 'ð',\n",
    "    'SH': 'ʃ',\n",
    "    'ZH': 'ʒ',\n",
    "    'NG': 'ŋ',\n",
    "    'Y': 'j'\n",
    "})\n",
    "\n",
    "def ipa_to_arpabet(ipa_string):\n",
    "    \"\"\"\n",
    "    Convert IPA phoneme string to ARPABET format.\n",
    "    \n",
    "    Args:\n",
    "        ipa_string (str): IPA phoneme string (space-separated or continuous)\n",
    "    \n",
    "    Returns:\n",
    "        str: ARPABET phoneme string (space-separated)\n",
    "    \"\"\"\n",
    "    if not ipa_string:\n",
    "        return \"\"\n",
    "    \n",
    "    # Handle both space-separated and continuous IPA strings\n",
    "    result = []\n",
    "    i = 0\n",
    "    \n",
    "    while i < len(ipa_string):\n",
    "        # Try to match longer sequences first (for diphthongs and affricates)\n",
    "        matched = False\n",
    "        \n",
    "        # Check for 3-character sequences\n",
    "        if i + 2 < len(ipa_string):\n",
    "            three_char = ipa_string[i:i+3]\n",
    "            if three_char in IPA_TO_ARPABET:\n",
    "                arpabet = IPA_TO_ARPABET[three_char]\n",
    "                if arpabet:\n",
    "                    result.append(arpabet)\n",
    "                i += 3\n",
    "                matched = True\n",
    "        \n",
    "        # Check for 2-character sequences\n",
    "        if not matched and i + 1 < len(ipa_string):\n",
    "            two_char = ipa_string[i:i+2]\n",
    "            if two_char in IPA_TO_ARPABET:\n",
    "                arpabet = IPA_TO_ARPABET[two_char]\n",
    "                if arpabet:\n",
    "                    result.append(arpabet)\n",
    "                i += 2\n",
    "                matched = True\n",
    "        \n",
    "        # Check for single character\n",
    "        if not matched:\n",
    "            char = ipa_string[i]\n",
    "            if char in IPA_TO_ARPABET:\n",
    "                arpabet = IPA_TO_ARPABET[char]\n",
    "                if arpabet:\n",
    "                    result.append(arpabet)\n",
    "            elif char == ' ':\n",
    "                # Handle word boundaries\n",
    "                if result and result[-1] != '|':\n",
    "                    result.append('|')  # Word boundary marker\n",
    "            i += 1\n",
    "    \n",
    "    return ' '.join(result)\n",
    "\n",
    "def arpabet_to_ipa(arpabet_string):\n",
    "    \"\"\"\n",
    "    Convert ARPABET phoneme string to IPA format.\n",
    "    \n",
    "    Args:\n",
    "        arpabet_string (str): ARPABET phoneme string (space-separated)\n",
    "    \n",
    "    Returns:\n",
    "        str: IPA phoneme string\n",
    "    \"\"\"\n",
    "    if not arpabet_string:\n",
    "        return \"\"\n",
    "    \n",
    "    # Split by spaces and convert each phoneme\n",
    "    phonemes = arpabet_string.strip().split()\n",
    "    ipa_result = []\n",
    "    \n",
    "    for phoneme in phonemes:\n",
    "        # Remove stress markers (0, 1, 2) from vowels\n",
    "        clean_phoneme = phoneme.rstrip('012')\n",
    "        \n",
    "        if clean_phoneme in ARPABET_TO_IPA:\n",
    "            ipa_result.append(ARPABET_TO_IPA[clean_phoneme])\n",
    "        elif clean_phoneme == '|':\n",
    "            ipa_result.append(' ')  # Word boundary\n",
    "        else:\n",
    "            # If phoneme not found, keep as is (might be a variant)\n",
    "            ipa_result.append(clean_phoneme.lower())\n",
    "    \n",
    "    return ''.join(ipa_result)\n",
    "\n",
    "def convert_phonemes_for_alignment(phoneme_results, output_format='arpabet'):\n",
    "    \"\"\"\n",
    "    Convert phoneme results to specified format for force alignment.\n",
    "    \n",
    "    Args:\n",
    "        phoneme_results (list): List of (word, phonemes) tuples\n",
    "        output_format (str): 'arpabet' or 'ipa'\n",
    "    \n",
    "    Returns:\n",
    "        list: List of (word, converted_phonemes) tuples\n",
    "    \"\"\"\n",
    "    converted_results = []\n",
    "    \n",
    "    for word, phonemes in phoneme_results:\n",
    "        if output_format.lower() == 'arpabet':\n",
    "            # Assume input is IPA, convert to ARPABET\n",
    "            converted = ipa_to_arpabet(phonemes)\n",
    "        elif output_format.lower() == 'ipa':\n",
    "            # Assume input is ARPABET, convert to IPA\n",
    "            converted = arpabet_to_ipa(phonemes)\n",
    "        else:\n",
    "            converted = phonemes  # No conversion\n",
    "        \n",
    "        converted_results.append((word, converted))\n",
    "    \n",
    "    return converted_results\n",
    "\n",
    "def display_phoneme_comparison(word, ipa_phonemes, arpabet_phonemes):\n",
    "    \"\"\"\n",
    "    Display a comparison of IPA and ARPABET phonemes for a word.\n",
    "    \n",
    "    Args:\n",
    "        word (str): The word\n",
    "        ipa_phonemes (str): IPA phoneme representation\n",
    "        arpabet_phonemes (str): ARPABET phoneme representation\n",
    "    \"\"\"\n",
    "    print(f\"Word: '{word}'\")\n",
    "    print(f\"  IPA:     [{ipa_phonemes}]\")\n",
    "    print(f\"  ARPABET: [{arpabet_phonemes}]\")\n",
    "    print(\"-\" * 40)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d23ee2eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original transcript: How are you? What are you doing? I would like\n",
      "Number of words: 10\n",
      "------------------------------------------------------------\n",
      " 1. Word: 'how' -> Phonemes: [ h ˈaʊ]\n",
      " 2. Word: 'are' -> Phonemes: [ ə]\n",
      " 3. Word: 'you' -> Phonemes: [ j ə]\n",
      " 4. Word: 'what' -> Phonemes: [ w ˈɒ t]\n",
      " 5. Word: 'are' -> Phonemes: [ ə]\n",
      " 6. Word: 'you' -> Phonemes: [ j ə]\n",
      " 7. Word: 'doing' -> Phonemes: [ d ˈuː ɪ ŋ]\n",
      " 8. Word: 'i' -> Phonemes: [ ˈaɪ]\n",
      " 9. Word: 'would' -> Phonemes: [ w ˈʊ d]\n",
      "10. Word: 'like' -> Phonemes: [ l ˈaɪ k]\n",
      "------------------------------------------------------------\n",
      "Complete phoneme sequence:\n",
      " h ˈaʊ |  ə |  j ə |  w ˈɒ t |  ə |  j ə |  d ˈuː ɪ ŋ |  ˈaɪ |  w ˈʊ d |  l ˈaɪ k\n",
      "Word: 'how'\n",
      "  IPA:     [ h ˈaʊ]\n",
      "  ARPABET: [  HH   AW]\n",
      "----------------------------------------\n",
      "Word: 'are'\n",
      "  IPA:     [ ə]\n",
      "  ARPABET: [  AH]\n",
      "----------------------------------------\n",
      "Word: 'you'\n",
      "  IPA:     [ j ə]\n",
      "  ARPABET: [  Y   AH]\n",
      "----------------------------------------\n",
      "Word: 'what'\n",
      "  IPA:     [ w ˈɒ t]\n",
      "  ARPABET: [  W     T]\n",
      "----------------------------------------\n",
      "Word: 'are'\n",
      "  IPA:     [ ə]\n",
      "  ARPABET: [  AH]\n",
      "----------------------------------------\n",
      "Word: 'you'\n",
      "  IPA:     [ j ə]\n",
      "  ARPABET: [  Y   AH]\n",
      "----------------------------------------\n",
      "Word: 'doing'\n",
      "  IPA:     [ d ˈuː ɪ ŋ]\n",
      "  ARPABET: [  D   UW   IH   NG]\n",
      "----------------------------------------\n",
      "Word: 'i'\n",
      "  IPA:     [ ˈaɪ]\n",
      "  ARPABET: [  AY]\n",
      "----------------------------------------\n",
      "Word: 'would'\n",
      "  IPA:     [ w ˈʊ d]\n",
      "  ARPABET: [  W   UH   D]\n",
      "----------------------------------------\n",
      "Word: 'like'\n",
      "  IPA:     [ l ˈaɪ k]\n",
      "  ARPABET: [  L   AY   K]\n",
      "----------------------------------------\n",
      "[('how', ' h ˈaʊ'), ('are', ' ə'), ('you', ' j ə'), ('what', ' w ˈɒ t'), ('are', ' ə'), ('you', ' j ə'), ('doing', ' d ˈuː ɪ ŋ'), ('i', ' ˈaɪ'), ('would', ' w ˈʊ d'), ('like', ' l ˈaɪ k')]\n"
     ]
    }
   ],
   "source": [
    "# Let's test the conversion functions\n",
    "if 'transcript' in locals() and transcript:\n",
    "    # Process the first few words for demonstration\n",
    "    sample_phonemes = transcript_to_phonemes(transcript.split('.')[0], britfone_lexicon)\n",
    "    \n",
    "    # Convert to ARPABET\n",
    "    arpabet_results = convert_phonemes_for_alignment(sample_phonemes, 'arpabet')\n",
    "    \n",
    "    # Display comparison\n",
    "    for i, (word, ipa_phonemes) in enumerate(sample_phonemes):\n",
    "        arpabet_phonemes = arpabet_results[i][1]\n",
    "        display_phoneme_comparison(word, ipa_phonemes, arpabet_phonemes)\n",
    "else:\n",
    "    print(\"Transcript not available. Please run the audio processing cells first.\")\n",
    "\n",
    "print(sample_phonemes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34bd09f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
