{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39bab11b",
   "metadata": {},
   "source": [
    "# GenIELTS: Phoneme Error Detection and Correction System - Action Plan\n",
    "\n",
    "This notebook implements the 5-day action plan to develop a prototype of the GenIELTS system."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cfdfb7",
   "metadata": {},
   "source": [
    "## Day 1: Environment Setup and ASR Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf4cbd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\abdal\\miniconda3\\envs\\mfa-env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 4060 Laptop GPU\n",
      "GPU Memory: 8.0 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of Wav2Vec2ForCTC were not initialized from the model checkpoint at facebook/wav2vec2-large-960h-lv60-self and are newly initialized: ['wav2vec2.masked_spec_embed']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded on: cuda:0\n",
      "Successfully loaded: Wav2Vec2FABundle(_path='https://dl.fbaipublicfiles.com/mms/torchaudio/ctc_alignment_mling_uroman/model.pt', _params={'extractor_mode': 'layer_norm', 'extractor_conv_layer_config': [(512, 10, 5), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 2, 2), (512, 2, 2)], 'extractor_conv_bias': True, 'encoder_embed_dim': 1024, 'encoder_projection_dropout': 0.0, 'encoder_pos_conv_kernel': 128, 'encoder_pos_conv_groups': 16, 'encoder_num_layers': 24, 'encoder_num_heads': 16, 'encoder_attention_dropout': 0.0, 'encoder_ff_interm_features': 4096, 'encoder_ff_interm_dropout': 0.1, 'encoder_dropout': 0.0, 'encoder_layer_norm_first': True, 'encoder_layer_drop': 0.1, 'aux_num_out': 28}, _sample_rate=16000, _normalize_waveform=True, _model_type='Wav2Vec2', _labels=('a', 'i', 'e', 'n', 'o', 'u', 't', 's', 'r', 'm', 'k', 'l', 'd', 'g', 'h', 'y', 'b', 'p', 'w', 'c', 'v', 'j', 'z', 'f', \"'\", 'q', 'x'), _remove_aux_axis=(1, 2, 3))\n",
      "Forced alignment model loaded on: cuda:0\n",
      "Successfully loaded: Wav2Vec2FABundle(_path='https://dl.fbaipublicfiles.com/mms/torchaudio/ctc_alignment_mling_uroman/model.pt', _params={'extractor_mode': 'layer_norm', 'extractor_conv_layer_config': [(512, 10, 5), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 3, 2), (512, 2, 2), (512, 2, 2)], 'extractor_conv_bias': True, 'encoder_embed_dim': 1024, 'encoder_projection_dropout': 0.0, 'encoder_pos_conv_kernel': 128, 'encoder_pos_conv_groups': 16, 'encoder_num_layers': 24, 'encoder_num_heads': 16, 'encoder_attention_dropout': 0.0, 'encoder_ff_interm_features': 4096, 'encoder_ff_interm_dropout': 0.1, 'encoder_dropout': 0.0, 'encoder_layer_norm_first': True, 'encoder_layer_drop': 0.1, 'aux_num_out': 28}, _sample_rate=16000, _normalize_waveform=True, _model_type='Wav2Vec2', _labels=('a', 'i', 'e', 'n', 'o', 'u', 't', 's', 'r', 'm', 'k', 'l', 'd', 'g', 'h', 'y', 'b', 'p', 'w', 'c', 'v', 'j', 'z', 'f', \"'\", 'q', 'x'), _remove_aux_axis=(1, 2, 3))\n",
      "Forced alignment model loaded on: cuda:0\n"
     ]
    }
   ],
   "source": [
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "import librosa\n",
    "import torch\n",
    "import torchaudio\n",
    "\n",
    "# Check for GPU availability\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "\n",
    "# Load pre-trained model and processor for transcription\n",
    "model_name = \"facebook/wav2vec2-large-960h-lv60-self\"\n",
    "processor = Wav2Vec2Processor.from_pretrained(model_name, sampling_rate=16000)\n",
    "model = Wav2Vec2ForCTC.from_pretrained(model_name)\n",
    "\n",
    "# Move model to GPU if available\n",
    "model = model.to(device)\n",
    "print(f\"Model loaded on: {next(model.parameters()).device}\")\n",
    "\n",
    "# Load forced alignment bundle - try different pipeline names\n",
    "fa_bundle = None\n",
    "fa_model = None\n",
    "fa_tokenizer = None\n",
    "\n",
    "pipeline_names = [\n",
    "    \"MMS_FA\",\n",
    "    \"WAV2VEC2_FA_BUNDLE\",\n",
    "    \"WAV2VEC2_ASR_BASE_960H\", \n",
    "]\n",
    "\n",
    "for name in pipeline_names:\n",
    "    try:\n",
    "        bundle = getattr(torchaudio.pipelines, name, None)\n",
    "        if bundle is not None:\n",
    "            fa_model = bundle.get_model()\n",
    "            fa_tokenizer = bundle.get_tokenizer()\n",
    "            fa_bundle = bundle\n",
    "            # Move forced alignment model to GPU as well\n",
    "            fa_model = fa_model.to(device)\n",
    "            print(f\"Successfully loaded: {bundle}\")\n",
    "            print(f\"Forced alignment model loaded on: {next(fa_model.parameters()).device}\")\n",
    "            break\n",
    "    except Exception as e:\n",
    "        continue\n",
    "\n",
    "if fa_bundle is None:\n",
    "    print(\"Warning: Could not load any forced alignment bundle. Will use fallback alignment.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2b07f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing alternative G2P:\n",
      "'hello' ‚Üí HH AH0 L OW1\n",
      "'world' ‚Üí W ER1 L D\n",
      "'pronunciation' ‚Üí P R OW0 N AH2 N S IY0 EY1 SH AH0 N\n",
      "'assessment' ‚Üí AH0 S EH1 S M AH0 N T\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats\n",
    "from scipy.signal import find_peaks\n",
    "from scipy.ndimage import median_filter\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# Alternative G2P using transformer-based approach\n",
    "from g2p_en import G2p\n",
    "\n",
    "# Initialize the G2P converter\n",
    "g2p = G2p()\n",
    "\n",
    "def alternative_g2p(word):\n",
    "    \"\"\"\n",
    "    Alternative G2P using g2p-en library (doesn't require espeak).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        phonemes = g2p(word)\n",
    "        # Convert to a format similar to britfone\n",
    "        return ' '.join(phonemes)\n",
    "    except Exception as e:\n",
    "        print(f\"G2P failed for '{word}': {e}\")\n",
    "        return word.lower()\n",
    "\n",
    "def get_phonemes_enhanced(word, lexicon):\n",
    "    \"\"\"\n",
    "    Enhanced phoneme lookup with multiple fallback options.\n",
    "    \"\"\"\n",
    "    word = word.lower()\n",
    "    if word in lexicon:\n",
    "        return lexicon[word][0]  # Return the first pronunciation from Britfone\n",
    "    else:\n",
    "        # Try alternative G2P first\n",
    "        try:\n",
    "            return alternative_g2p(word)\n",
    "        except Exception as e:\n",
    "            print(f\"Alternative G2P failed for '{word}': {e}\")\n",
    "            # Final fallback - simple phonetic approximation\n",
    "            return word.lower()\n",
    "\n",
    "# Test the alternative G2P\n",
    "print(\"Testing alternative G2P:\")\n",
    "test_words = [\"hello\", \"world\", \"pronunciation\", \"assessment\"]\n",
    "for word in test_words:\n",
    "    result = alternative_g2p(word)\n",
    "    print(f\"'{word}' ‚Üí {result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb52d7ee",
   "metadata": {},
   "source": [
    "## Day 2: British English Pronunciation Lexicon and Grapheme-to-Phoneme (G2P)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c107750e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import re\n",
    "\n",
    "def parse_britfone(file_path):\n",
    "    \"\"\"\n",
    "    Parses the Britfone csv file into a dictionary.\n",
    "    \"\"\"\n",
    "    pronunciations = {}\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        next(reader) # skip header\n",
    "        for row in reader:\n",
    "            word = row[0].lower()\n",
    "            phonemes = row[1]\n",
    "            \n",
    "            # Handle multiple pronunciations (e.g., word(1))\n",
    "            word = re.sub(r'\\(\\d+\\)', '', word)\n",
    "            \n",
    "            if word in pronunciations:\n",
    "                if phonemes not in pronunciations[word]:\n",
    "                    pronunciations[word].append(phonemes)\n",
    "            else:\n",
    "                pronunciations[word] = [phonemes]\n",
    "    return pronunciations\n",
    "\n",
    "# Parse the downloaded Britfone file\n",
    "britfone_lexicon = parse_britfone(\"britfone.main.3.0.1.csv\")\n",
    "\n",
    "# Example usage\n",
    "# print(britfone_lexicon.get('fox'))\n",
    "# print(britfone_lexicon.get('fork'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c920451f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91edddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "import torchaudio.functional as F_audio\n",
    "\n",
    "def get_phoneme_timings(file_path, lexicon):\n",
    "    \"\"\"\n",
    "    Extracts phoneme timings from an audio file using Wav2Vec2 forced alignment.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio file with torchaudio (required for the FA model)\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        \n",
    "        # Move waveform to the same device as the model\n",
    "        waveform = waveform.to(device)\n",
    "        \n",
    "        # Resample if necessary\n",
    "        if fa_bundle and hasattr(fa_bundle, 'sample_rate') and sample_rate != fa_bundle.sample_rate:\n",
    "            waveform = F_audio.resample(waveform, sample_rate, fa_bundle.sample_rate)\n",
    "        \n",
    "        # First get transcription using the original model\n",
    "        audio_librosa, _ = librosa.load(file_path, sr=16000)\n",
    "        input_values = processor(audio_librosa, return_tensors=\"pt\", padding=\"longest\").input_values\n",
    "        \n",
    "        # Move input_values to the same device as the model\n",
    "        input_values = input_values.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            logits = model(input_values).logits\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        # Get word-level phonemes\n",
    "        words = transcription.split()\n",
    "        expected_phonemes_list = [get_phonemes_enhanced(word, lexicon) for word in words]\n",
    "\n",
    "        # Use forced alignment for accurate word timestamps\n",
    "        aligned_phonemes = transformer_forced_alignment(\n",
    "            waveform, transcription, expected_phonemes_list, words, lexicon\n",
    "        )\n",
    "\n",
    "        return aligned_phonemes, transcription\n",
    "\n",
    "    except Exception as e:\n",
    "        return f\"Error processing audio file: {e}\", \"\"\n",
    "\n",
    "def transformer_forced_alignment(waveform, transcription, expected_phonemes_list, words, lexicon):\n",
    "    \"\"\"\n",
    "    Performs forced alignment using available torchaudio model to get accurate word timestamps.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Input validation\n",
    "        if not transcription or not words:\n",
    "            print(\"Warning: Empty transcription or word list, using fallback alignment\")\n",
    "            return simple_phoneme_alignment(waveform, expected_phonemes_list, words)\n",
    "        \n",
    "        if not isinstance(transcription, str):\n",
    "            transcription = str(transcription)\n",
    "        \n",
    "        # Check if we have a proper tokenizer\n",
    "        if fa_bundle and hasattr(fa_bundle, 'get_tokenizer'):\n",
    "            try:\n",
    "                # Ensure transcription is a string and not empty\n",
    "                if not transcription or not isinstance(transcription, str):\n",
    "                    transcription = \" \".join(words) if words else \"hello\"\n",
    "                \n",
    "                tokens = fa_tokenizer(transcription)\n",
    "            except Exception as tokenizer_error:\n",
    "                print(f\"Tokenizer error with '{transcription}': {tokenizer_error}\")\n",
    "                # Fallback: create simple tokens based on words\n",
    "                tokens = list(range(len(words))) if words else [0]\n",
    "        else:\n",
    "            # Use character-level tokenization as fallback\n",
    "            if hasattr(fa_tokenizer, '__iter__') and not isinstance(fa_tokenizer, str):\n",
    "                labels = list(fa_tokenizer)  # Convert to list if it's iterable\n",
    "            else:\n",
    "                # Create a simple alphabet if tokenizer doesn't have labels\n",
    "                labels = list(\"ABCDEFGHIJKLMNOPQRSTUVWXYZ \")\n",
    "            \n",
    "            char_to_idx = {char: idx for idx, char in enumerate(labels)}\n",
    "            tokens = [char_to_idx.get(char.upper(), 0) for char in transcription if char.isalpha() or char.isspace()]\n",
    "        \n",
    "        # Get emission from the forced alignment model\n",
    "        with torch.no_grad():\n",
    "            if hasattr(fa_model, '__call__'):\n",
    "                # Ensure waveform is on the same device as fa_model\n",
    "                if fa_model is not None:\n",
    "                    fa_device = next(fa_model.parameters()).device\n",
    "                    waveform = waveform.to(fa_device)\n",
    "                \n",
    "                # Ensure waveform has the correct shape for forced alignment model\n",
    "                # FA model expects 2D tensor: (batch_size, sequence_length)\n",
    "                if waveform.dim() == 3:\n",
    "                    # Shape: (batch, channels, length) -> squeeze out channel dimension\n",
    "                    waveform = waveform.squeeze(1)  # Shape: (batch, length)\n",
    "                elif waveform.dim() == 2 and waveform.size(0) == 1:\n",
    "                    # Shape: (1, length) -> keep as is, this is correct\n",
    "                    pass\n",
    "                elif waveform.dim() == 2 and waveform.size(0) > 1:\n",
    "                    # Shape: (channels, length) where channels > 1 -> take first channel and add batch dim\n",
    "                    waveform = waveform[0:1]  # Shape: (1, length)\n",
    "                elif waveform.dim() == 1:\n",
    "                    # Shape: (length,) -> add batch dimension\n",
    "                    waveform = waveform.unsqueeze(0)  # Shape: (1, length)\n",
    "                \n",
    "                print(f\"Final waveform shape for FA model: {waveform.shape}\")\n",
    "                \n",
    "                try:\n",
    "                    emission, _ = fa_model(waveform)\n",
    "                except Exception as model_error:\n",
    "                    print(f\"FA model error: {model_error}\")\n",
    "                    # If still failing, try alternative approaches\n",
    "                    if waveform.dim() == 2:\n",
    "                        # Try with just the audio sequence (remove batch dimension)\n",
    "                        waveform_1d = waveform.squeeze(0)  # Shape: (length,)\n",
    "                        print(f\"Trying 1D waveform shape: {waveform_1d.shape}\")\n",
    "                        # Add batch dimension back\n",
    "                        waveform_1d = waveform_1d.unsqueeze(0)  # Shape: (1, length)\n",
    "                        emission, _ = fa_model(waveform_1d)\n",
    "                    else:\n",
    "                        raise model_error\n",
    "            else:\n",
    "                # Alternative approach if model structure is different\n",
    "                emission = fa_model(waveform)\n",
    "                if isinstance(emission, tuple):\n",
    "                    emission = emission[0]\n",
    "        \n",
    "        # Try to perform forced alignment\n",
    "        try:\n",
    "            if hasattr(F_audio, 'forced_align') and len(tokens) > 0:\n",
    "                # Ensure tokens is a proper tensor on the same device as emission\n",
    "                if isinstance(tokens, list):\n",
    "                    tokens = torch.tensor(tokens)\n",
    "                if hasattr(emission, 'device'):\n",
    "                    tokens = tokens.to(emission.device)\n",
    "                alignments, scores = F_audio.forced_align(emission, tokens, blank=0)\n",
    "            else:\n",
    "                # Fallback: use CTC beam search decoder\n",
    "                from torch.nn import functional as F_nn\n",
    "                log_probs = F_nn.log_softmax(emission, dim=-1)\n",
    "                # Simple argmax decoding as fallback\n",
    "                alignments = [(torch.argmax(log_probs[0, i]).item(), i, i+1) for i in range(log_probs.size(1))]\n",
    "        except Exception as align_error:\n",
    "            print(f\"Forced alignment failed: {align_error}\")\n",
    "            print(f\"Debug info - transcription: '{transcription}', tokens: {tokens}\")\n",
    "            return simple_phoneme_alignment(waveform, expected_phonemes_list, words)\n",
    "        \n",
    "        # Convert frame indices to time\n",
    "        sample_rate = getattr(fa_bundle, 'sample_rate', 16000) if fa_bundle else 16000\n",
    "        ratio = waveform.size(1) / emission.size(1) / sample_rate\n",
    "        word_timestamps = []\n",
    "        \n",
    "        # Simple approach: divide audio duration equally among words\n",
    "        total_duration = waveform.size(1) / sample_rate\n",
    "        word_duration = total_duration / len(words) if words else 0\n",
    "        \n",
    "        for i, word in enumerate(words):\n",
    "            word_timestamps.append({\n",
    "                'word': word,\n",
    "                'start': i * word_duration,\n",
    "                'end': (i + 1) * word_duration\n",
    "            })\n",
    "        \n",
    "        # Now distribute word durations among phonemes\n",
    "        aligned_phonemes = []\n",
    "        \n",
    "        for i, word_info in enumerate(word_timestamps):\n",
    "            if i < len(expected_phonemes_list):\n",
    "                phonemes = expected_phonemes_list[i].split()\n",
    "                if not phonemes:\n",
    "                    continue\n",
    "                \n",
    "                word_duration = word_info['end'] - word_info['start']\n",
    "                phoneme_duration = word_duration / len(phonemes) if phonemes else 0\n",
    "                \n",
    "                current_time = word_info['start']\n",
    "                for phoneme in phonemes:\n",
    "                    aligned_phonemes.append({\n",
    "                        'phoneme': phoneme,\n",
    "                        'start': current_time,\n",
    "                        'end': current_time + phoneme_duration,\n",
    "                        'score': 0.8  # Moderate confidence for this approach\n",
    "                    })\n",
    "                    current_time += phoneme_duration\n",
    "        \n",
    "        return aligned_phonemes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Transformer alignment failed: {e}\")\n",
    "        print(f\"Debug info:\")\n",
    "        print(f\"  - Waveform shape: {waveform.shape}\")\n",
    "        print(f\"  - Transcription: '{transcription}'\")\n",
    "        print(f\"  - Number of words: {len(words)}\")\n",
    "        print(f\"  - FA model available: {fa_model is not None}\")\n",
    "        # Fallback to simple duration-based alignment\n",
    "        return simple_phoneme_alignment(waveform, expected_phonemes_list, words)\n",
    "\n",
    "def simple_phoneme_alignment(waveform, expected_phonemes_list, words):\n",
    "    \"\"\"\n",
    "    Fallback simple alignment method.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        sample_rate = getattr(fa_bundle, 'sample_rate', 16000) if fa_bundle else 16000\n",
    "        \n",
    "        # Handle different waveform shapes\n",
    "        if waveform.dim() == 3:\n",
    "            # Shape: (batch, channels, length) -> use length from last dimension\n",
    "            total_duration = waveform.size(-1) / sample_rate\n",
    "        elif waveform.dim() == 2:\n",
    "            # Shape: (channels, length) -> use length from last dimension\n",
    "            total_duration = waveform.size(-1) / sample_rate\n",
    "        elif waveform.dim() == 1:\n",
    "            # Shape: (length,) -> use length\n",
    "            total_duration = waveform.size(0) / sample_rate\n",
    "        else:\n",
    "            # Fallback duration if we can't determine from waveform\n",
    "            total_duration = 5.0  # Assume 5 seconds\n",
    "        \n",
    "        aligned_phonemes = []\n",
    "        current_time = 0.0\n",
    "        \n",
    "        # Calculate total number of phonemes\n",
    "        total_phonemes = sum(len(phonemes.split()) for phonemes in expected_phonemes_list)\n",
    "        \n",
    "        if total_phonemes == 0:\n",
    "            return aligned_phonemes\n",
    "        \n",
    "        phoneme_duration = total_duration / total_phonemes\n",
    "        \n",
    "        for phoneme_seq in expected_phonemes_list:\n",
    "            phonemes = phoneme_seq.split()\n",
    "            for phoneme in phonemes:\n",
    "                aligned_phonemes.append({\n",
    "                    'phoneme': phoneme,\n",
    "                    'start': current_time,\n",
    "                    'end': current_time + phoneme_duration,\n",
    "                    'score': 0.5  # Lower confidence for fallback method\n",
    "                })\n",
    "                current_time += phoneme_duration\n",
    "        \n",
    "        return aligned_phonemes\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in simple alignment: {e}\")\n",
    "        # Final fallback: return empty list\n",
    "        return []\n",
    "\n",
    "def transcribe_audio(audio_path):\n",
    "    \"\"\"Transcribe audio using Wav2Vec2 model with GPU support.\"\"\"\n",
    "    try:\n",
    "        # Load audio file\n",
    "        waveform, sample_rate = librosa.load(audio_path, sr=16000)  # Explicit sr=16000\n",
    "        \n",
    "        # Process input\n",
    "        inputs = processor(waveform, sampling_rate=16000, return_tensors=\"pt\", padding=True)\n",
    "        \n",
    "        # Move inputs to the same device as the model\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Get model predictions\n",
    "        with torch.no_grad():\n",
    "            logits = model(**inputs).logits\n",
    "        \n",
    "        # Decode predictions\n",
    "        predicted_ids = torch.argmax(logits, dim=-1)\n",
    "        transcription = processor.batch_decode(predicted_ids)[0]\n",
    "        \n",
    "        return transcription.lower().strip()\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error in transcription: {e}\")\n",
    "        return \"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9dbcb57",
   "metadata": {},
   "source": [
    "## Day 4: Phoneme Comparison and Error Scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dff3386f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import Levenshtein\n",
    "\n",
    "def compare_phonemes(expected_phonemes, actual_phonemes):\n",
    "    \"\"\"\n",
    "    Compares expected and actual phoneme sequences to identify errors.\n",
    "    \"\"\"\n",
    "    expected = expected_phonemes.split()\n",
    "    actual = [p['phoneme'] for p in actual_phonemes]\n",
    "    \n",
    "    # Using Levenshtein distance to find the edit operations\n",
    "    edits = Levenshtein.editops(expected, actual)\n",
    "    \n",
    "    errors = []\n",
    "    for edit_type, pos_expected, pos_actual in edits:\n",
    "        if edit_type == 'replace':\n",
    "            errors.append({\n",
    "                'type': 'Substitution',\n",
    "                'expected': expected[pos_expected],\n",
    "                'actual': actual[pos_actual],\n",
    "                'position': pos_expected\n",
    "            })\n",
    "        elif edit_type == 'delete':\n",
    "            errors.append({\n",
    "                'type': 'Deletion',\n",
    "                'expected': expected[pos_expected],\n",
    "                'actual': None,\n",
    "                'position': pos_expected\n",
    "            })\n",
    "        elif edit_type == 'insert':\n",
    "            errors.append({\n",
    "                'type': 'Insertion',\n",
    "                'expected': None,\n",
    "                'actual': actual[pos_actual],\n",
    "                'position': pos_expected\n",
    "            })\n",
    "            \n",
    "\n",
    "    \n",
    "    return errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "693e8c88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_gop_scores(audio_path, expected_phonemes, aligned_phonemes):\n",
    "    \"\"\"\n",
    "    Calculate Goodness of Pronunciation (GOP) scores for each phoneme.\n",
    "    GOP score indicates how well each phoneme was pronounced (0-1, higher is better).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio for analysis\n",
    "        waveform, sample_rate = torchaudio.load(audio_path)\n",
    "        waveform = waveform.to(device)\n",
    "        \n",
    "        # Get acoustic features using the ASR model\n",
    "        with torch.no_grad():\n",
    "            if hasattr(model, 'wav2vec2'):\n",
    "                # Extract features from wav2vec2 backbone\n",
    "                features = model.wav2vec2.feature_extractor(waveform)\n",
    "                features = model.wav2vec2.feature_projection(features.transpose(1, 2))\n",
    "            else:\n",
    "                # Fallback: use logits as features\n",
    "                inputs = processor(waveform.cpu().numpy().flatten(), \n",
    "                                 sampling_rate=sample_rate, \n",
    "                                 return_tensors=\"pt\", padding=True)\n",
    "                inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "                logits = model(**inputs).logits\n",
    "                features = torch.softmax(logits, dim=-1)\n",
    "        \n",
    "        gop_scores = []\n",
    "        expected_list = expected_phonemes.split()\n",
    "        \n",
    "        for i, phoneme_info in enumerate(aligned_phonemes):\n",
    "            if i >= len(expected_list):\n",
    "                break\n",
    "                \n",
    "            expected_phoneme = expected_list[i]\n",
    "            actual_phoneme = phoneme_info['phoneme']\n",
    "            \n",
    "            # Calculate GOP based on multiple factors\n",
    "            \n",
    "            # 1. Phoneme match score (exact match gets high score)\n",
    "            match_score = 1.0 if expected_phoneme == actual_phoneme else 0.3\n",
    "            \n",
    "            # 2. Acoustic confidence (from alignment score if available)\n",
    "            acoustic_score = phoneme_info.get('score', 0.5)\n",
    "            \n",
    "            # 3. Duration appropriateness (relative to expected duration)\n",
    "            duration = phoneme_info['end'] - phoneme_info['start']\n",
    "            expected_duration = 0.1  # Average phoneme duration baseline\n",
    "            duration_score = min(1.0, expected_duration / max(duration, 0.01))\n",
    "            \n",
    "            # Combine scores with weights\n",
    "            gop_score = (0.5 * match_score + \n",
    "                        0.3 * acoustic_score + \n",
    "                        0.2 * duration_score)\n",
    "            \n",
    "            gop_scores.append({\n",
    "                'phoneme': actual_phoneme,\n",
    "                'expected': expected_phoneme,\n",
    "                'gop_score': gop_score,\n",
    "                'match_score': match_score,\n",
    "                'acoustic_score': acoustic_score,\n",
    "                'duration_score': duration_score,\n",
    "                'start': phoneme_info['start'],\n",
    "                'end': phoneme_info['end']\n",
    "            })\n",
    "        \n",
    "        return gop_scores\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating GOP scores: {e}\")\n",
    "        # Fallback GOP calculation\n",
    "        fallback_scores = []\n",
    "        expected_list = expected_phonemes.split()\n",
    "        \n",
    "        for i, phoneme_info in enumerate(aligned_phonemes):\n",
    "            if i >= len(expected_list):\n",
    "                break\n",
    "            \n",
    "            expected_phoneme = expected_list[i]\n",
    "            actual_phoneme = phoneme_info['phoneme']\n",
    "            \n",
    "            # Simple fallback score\n",
    "            gop_score = 0.8 if expected_phoneme == actual_phoneme else 0.4\n",
    "            \n",
    "            fallback_scores.append({\n",
    "                'phoneme': actual_phoneme,\n",
    "                'expected': expected_phoneme,\n",
    "                'gop_score': gop_score,\n",
    "                'match_score': gop_score,\n",
    "                'acoustic_score': 0.5,\n",
    "                'duration_score': 0.5,\n",
    "                'start': phoneme_info['start'],\n",
    "                'end': phoneme_info['end']\n",
    "            })\n",
    "        \n",
    "        return fallback_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73e42d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_errors_to_words(errors, transcription, expected_phonemes_list):\n",
    "    \"\"\"\n",
    "    Maps phoneme-level errors to specific words for better feedback.\n",
    "    \"\"\"\n",
    "    words = transcription.split()\n",
    "    word_errors = []\n",
    "    \n",
    "    # Create mapping from phoneme position to word\n",
    "    phoneme_to_word = []\n",
    "    phoneme_pos = 0\n",
    "    \n",
    "    for word_idx, word in enumerate(words):\n",
    "        if word_idx < len(expected_phonemes_list):\n",
    "            phonemes_in_word = expected_phonemes_list[word_idx].split()\n",
    "            for _ in phonemes_in_word:\n",
    "                phoneme_to_word.append(word_idx)\n",
    "                phoneme_pos += 1\n",
    "    \n",
    "    # Group errors by word\n",
    "    word_error_groups = {}\n",
    "    for error in errors:\n",
    "        position = error.get('position', 0)\n",
    "        if position < len(phoneme_to_word):\n",
    "            word_idx = phoneme_to_word[position]\n",
    "            if word_idx not in word_error_groups:\n",
    "                word_error_groups[word_idx] = []\n",
    "            word_error_groups[word_idx].append(error)\n",
    "    \n",
    "    # Create word-level error summaries\n",
    "    for word_idx, word_errors_list in word_error_groups.items():\n",
    "        if word_idx < len(words):\n",
    "            word = words[word_idx]\n",
    "            error_types = [e['type'] for e in word_errors_list]\n",
    "            \n",
    "            word_errors.append({\n",
    "                'word': word,\n",
    "                'word_index': word_idx,\n",
    "                'errors': word_errors_list,\n",
    "                'error_count': len(word_errors_list),\n",
    "                'error_types': list(set(error_types)),\n",
    "                'severity': calculate_word_error_severity(word_errors_list)\n",
    "            })\n",
    "    \n",
    "    return word_errors\n",
    "\n",
    "def calculate_word_error_severity(word_errors_list):\n",
    "    \"\"\"\n",
    "    Calculate severity of errors for a word (0-1, higher is more severe).\n",
    "    \"\"\"\n",
    "    if not word_errors_list:\n",
    "        return 0.0\n",
    "    \n",
    "    severity_weights = {\n",
    "        'Substitution': 0.7,\n",
    "        'Deletion': 0.9,\n",
    "        'Insertion': 0.5\n",
    "    }\n",
    "    \n",
    "    total_severity = sum(severity_weights.get(error['type'], 0.5) \n",
    "                        for error in word_errors_list)\n",
    "    \n",
    "    # Normalize by number of errors\n",
    "    return min(1.0, total_severity / len(word_errors_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "063f060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_prosodic_features(audio_path):\n",
    "    \"\"\"\n",
    "    Analyze prosodic features including stress, intonation, and rhythm.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio with librosa for prosodic analysis\n",
    "        y, sr = librosa.load(audio_path, sr=22050)\n",
    "        \n",
    "        # 1. Fundamental frequency (F0) for intonation analysis\n",
    "        f0 = librosa.yin(y, fmin=librosa.note_to_hz('C2'), fmax=librosa.note_to_hz('C7'))\n",
    "        f0_smooth = median_filter(f0, size=5)  # Smooth F0 contour\n",
    "        \n",
    "        # Calculate intonation features\n",
    "        f0_mean = np.nanmean(f0_smooth[f0_smooth > 0])\n",
    "        f0_std = np.nanstd(f0_smooth[f0_smooth > 0])\n",
    "        f0_range = np.nanmax(f0_smooth) - np.nanmin(f0_smooth[f0_smooth > 0])\n",
    "        \n",
    "        # 2. Energy/intensity analysis for stress detection\n",
    "        rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]\n",
    "        \n",
    "        # 3. Spectral features for voice quality\n",
    "        spectral_centroids = librosa.feature.spectral_centroid(y=y, sr=sr)[0]\n",
    "        mfccs = librosa.feature.mfcc(y=y, sr=sr, n_mfcc=13)\n",
    "        \n",
    "        # 4. Rhythm and timing analysis\n",
    "        tempo, beats = librosa.beat.beat_track(y=y, sr=sr)\n",
    "        \n",
    "        # 5. Detect stressed syllables (simplified approach)\n",
    "        # Find peaks in energy that could indicate stress\n",
    "        energy_peaks, _ = find_peaks(rms, height=np.mean(rms) + 0.5 * np.std(rms))\n",
    "        \n",
    "        prosodic_features = {\n",
    "            'f0_mean': float(f0_mean) if not np.isnan(f0_mean) else 0.0,\n",
    "            'f0_std': float(f0_std) if not np.isnan(f0_std) else 0.0,\n",
    "            'f0_range': float(f0_range) if not np.isnan(f0_range) else 0.0,\n",
    "            'intonation_variability': float(f0_std / f0_mean) if f0_mean > 0 else 0.0,\n",
    "            'energy_mean': float(np.mean(rms)),\n",
    "            'energy_std': float(np.std(rms)),\n",
    "            'tempo': float(tempo),\n",
    "            'stress_points': len(energy_peaks),\n",
    "            'spectral_centroid_mean': float(np.mean(spectral_centroids)),\n",
    "            'voice_quality_score': calculate_voice_quality_score(mfccs, f0_smooth),\n",
    "            'rhythm_regularity': calculate_rhythm_regularity(beats, sr)\n",
    "        }\n",
    "        \n",
    "        return prosodic_features\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing prosodic features: {e}\")\n",
    "        return {\n",
    "            'f0_mean': 0.0, 'f0_std': 0.0, 'f0_range': 0.0,\n",
    "            'intonation_variability': 0.0, 'energy_mean': 0.0, 'energy_std': 0.0,\n",
    "            'tempo': 0.0, 'stress_points': 0, 'spectral_centroid_mean': 0.0,\n",
    "            'voice_quality_score': 0.5, 'rhythm_regularity': 0.5\n",
    "        }\n",
    "\n",
    "def calculate_voice_quality_score(mfccs, f0):\n",
    "    \"\"\"\n",
    "    Calculate a voice quality score based on MFCCs and F0 stability.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Voice quality based on MFCC variance and F0 stability\n",
    "        mfcc_stability = 1.0 / (1.0 + np.mean(np.std(mfccs, axis=1)))\n",
    "        f0_stability = 1.0 / (1.0 + np.nanstd(f0[f0 > 0]) / np.nanmean(f0[f0 > 0]))\n",
    "        \n",
    "        if np.isnan(f0_stability):\n",
    "            f0_stability = 0.5\n",
    "            \n",
    "        return (mfcc_stability + f0_stability) / 2.0\n",
    "    except:\n",
    "        return 0.5\n",
    "\n",
    "def calculate_rhythm_regularity(beats, sr):\n",
    "    \"\"\"\n",
    "    Calculate rhythm regularity based on beat intervals.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        if len(beats) < 3:\n",
    "            return 0.5\n",
    "        \n",
    "        beat_intervals = np.diff(beats) / sr\n",
    "        rhythm_std = np.std(beat_intervals)\n",
    "        rhythm_mean = np.mean(beat_intervals)\n",
    "        \n",
    "        # Regular rhythm has low coefficient of variation\n",
    "        regularity = 1.0 / (1.0 + rhythm_std / rhythm_mean) if rhythm_mean > 0 else 0.5\n",
    "        return regularity\n",
    "    except:\n",
    "        return 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b1212ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_sentence_fluency(audio_path, transcription, gop_scores, prosodic_features):\n",
    "    \"\"\"\n",
    "    Assess sentence-level fluency including speed, pauses, and overall coherence.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Load audio for fluency analysis\n",
    "        y, sr = librosa.load(audio_path, sr=22050)\n",
    "        duration = len(y) / sr\n",
    "        \n",
    "        # 1. Speaking rate (words per minute)\n",
    "        word_count = len(transcription.split())\n",
    "        speaking_rate = (word_count / duration) * 60 if duration > 0 else 0\n",
    "        \n",
    "        # 2. Pause analysis\n",
    "        # Detect silence/pauses using energy threshold\n",
    "        rms = librosa.feature.rms(y=y, frame_length=2048, hop_length=512)[0]\n",
    "        silence_threshold = np.mean(rms) * 0.1\n",
    "        silence_frames = rms < silence_threshold\n",
    "        \n",
    "        # Convert frames to time\n",
    "        frame_times = librosa.frames_to_time(np.arange(len(rms)), sr=sr, hop_length=512)\n",
    "        pause_starts = []\n",
    "        pause_ends = []\n",
    "        in_pause = False\n",
    "        \n",
    "        for i, is_silent in enumerate(silence_frames):\n",
    "            if is_silent and not in_pause:\n",
    "                pause_starts.append(frame_times[i])\n",
    "                in_pause = True\n",
    "            elif not is_silent and in_pause:\n",
    "                pause_ends.append(frame_times[i])\n",
    "                in_pause = False\n",
    "        \n",
    "        # Calculate pause statistics\n",
    "        pause_durations = []\n",
    "        for start, end in zip(pause_starts, pause_ends):\n",
    "            duration_pause = end - start\n",
    "            if duration_pause > 0.1:  # Only count pauses longer than 100ms\n",
    "                pause_durations.append(duration_pause)\n",
    "        \n",
    "        avg_pause_duration = np.mean(pause_durations) if pause_durations else 0\n",
    "        total_pause_time = sum(pause_durations)\n",
    "        pause_frequency = len(pause_durations) / duration if duration > 0 else 0\n",
    "        \n",
    "        # 3. Pronunciation consistency (based on GOP scores)\n",
    "        if gop_scores:\n",
    "            avg_gop = np.mean([score['gop_score'] for score in gop_scores])\n",
    "            gop_std = np.std([score['gop_score'] for score in gop_scores])\n",
    "            pronunciation_consistency = 1.0 - gop_std  # Lower variance = higher consistency\n",
    "        else:\n",
    "            avg_gop = 0.5\n",
    "            pronunciation_consistency = 0.5\n",
    "        \n",
    "        # 4. Overall fluency score calculation\n",
    "        # Normalize speaking rate (optimal range: 150-200 WPM for clear speech)\n",
    "        rate_score = 1.0 - abs(speaking_rate - 175) / 175 if speaking_rate > 0 else 0\n",
    "        rate_score = max(0, min(1, rate_score))\n",
    "        \n",
    "        # Pause appropriateness (not too many, not too long)\n",
    "        pause_score = 1.0 / (1.0 + pause_frequency * 2)  # Penalty for too many pauses\n",
    "        pause_score *= 1.0 / (1.0 + avg_pause_duration)  # Penalty for long pauses\n",
    "        \n",
    "        # Prosodic naturalness\n",
    "        prosodic_score = (prosodic_features['voice_quality_score'] + \n",
    "                         prosodic_features['rhythm_regularity']) / 2\n",
    "        \n",
    "        # Combined fluency score\n",
    "        fluency_score = (0.3 * rate_score + \n",
    "                        0.25 * pause_score + \n",
    "                        0.25 * pronunciation_consistency + \n",
    "                        0.2 * prosodic_score)\n",
    "        \n",
    "        fluency_assessment = {\n",
    "            'overall_fluency_score': fluency_score,\n",
    "            'speaking_rate_wpm': speaking_rate,\n",
    "            'speaking_rate_score': rate_score,\n",
    "            'avg_pause_duration': avg_pause_duration,\n",
    "            'pause_frequency': pause_frequency,\n",
    "            'pause_score': pause_score,\n",
    "            'total_pause_time': total_pause_time,\n",
    "            'pause_count': len(pause_durations),\n",
    "            'pronunciation_consistency': pronunciation_consistency,\n",
    "            'avg_gop_score': avg_gop,\n",
    "            'prosodic_naturalness': prosodic_score,\n",
    "            'audio_duration': duration,\n",
    "            'word_count': word_count,\n",
    "            'fluency_level': get_fluency_level(fluency_score)\n",
    "        }\n",
    "        \n",
    "        return fluency_assessment\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error assessing fluency: {e}\")\n",
    "        return {\n",
    "            'overall_fluency_score': 0.5,\n",
    "            'speaking_rate_wpm': 0,\n",
    "            'speaking_rate_score': 0.5,\n",
    "            'avg_pause_duration': 0,\n",
    "            'pause_frequency': 0,\n",
    "            'pause_score': 0.5,\n",
    "            'total_pause_time': 0,\n",
    "            'pause_count': 0,\n",
    "            'pronunciation_consistency': 0.5,\n",
    "            'avg_gop_score': 0.5,\n",
    "            'prosodic_naturalness': 0.5,\n",
    "            'audio_duration': 0,\n",
    "            'word_count': 0,\n",
    "            'fluency_level': 'Intermediate'\n",
    "        }\n",
    "\n",
    "def get_fluency_level(fluency_score):\n",
    "    \"\"\"\n",
    "    Convert fluency score to IELTS-like level description.\n",
    "    \"\"\"\n",
    "    if fluency_score >= 0.85:\n",
    "        return \"Advanced (IELTS 7-9)\"\n",
    "    elif fluency_score >= 0.7:\n",
    "        return \"Upper-Intermediate (IELTS 6-7)\"\n",
    "    elif fluency_score >= 0.55:\n",
    "        return \"Intermediate (IELTS 5-6)\"\n",
    "    elif fluency_score >= 0.4:\n",
    "        return \"Lower-Intermediate (IELTS 4-5)\"\n",
    "    else:\n",
    "        return \"Beginner (IELTS 3-4)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "96174f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_enhanced_feedback(errors, word_errors, gop_scores, prosodic_features, fluency_assessment, transcription):\n",
    "    \"\"\"\n",
    "    Generates comprehensive, enhanced feedback using all analysis components.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"üéØ ENHANCED GenIELTS PRONUNCIATION ASSESSMENT\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Overall fluency assessment\n",
    "    print(f\"\\nüìä OVERALL FLUENCY ASSESSMENT\")\n",
    "    print(\"-\" * 35)\n",
    "    fluency_score = fluency_assessment['overall_fluency_score']\n",
    "    fluency_level = fluency_assessment['fluency_level']\n",
    "    \n",
    "    print(f\"üèÜ Fluency Score: {fluency_score:.2f}/1.00 ({fluency_level})\")\n",
    "    print(f\"‚è±Ô∏è Speaking Rate: {fluency_assessment['speaking_rate_wpm']:.1f} WPM\")\n",
    "    print(f\"‚è∏Ô∏è Pause Analysis: {fluency_assessment['pause_count']} pauses, avg {fluency_assessment['avg_pause_duration']:.2f}s\")\n",
    "    print(f\"üéØ Pronunciation Consistency: {fluency_assessment['pronunciation_consistency']:.2f}/1.00\")\n",
    "    \n",
    "    # GOP-based pronunciation quality\n",
    "    if gop_scores:\n",
    "        print(f\"\\nüéØ PRONUNCIATION QUALITY (GOP Analysis)\")\n",
    "        print(\"-\" * 42)\n",
    "        avg_gop = np.mean([score['gop_score'] for score in gop_scores])\n",
    "        print(f\"üìà Average GOP Score: {avg_gop:.2f}/1.00\")\n",
    "        \n",
    "        # Identify problematic phonemes\n",
    "        poor_phonemes = [score for score in gop_scores if score['gop_score'] < 0.6]\n",
    "        if poor_phonemes:\n",
    "            print(f\"‚ö†Ô∏è Phonemes needing attention ({len(poor_phonemes)} found):\")\n",
    "            for phoneme in poor_phonemes[:5]:  # Show top 5 issues\n",
    "                print(f\"   /{phoneme['expected']}/ ‚Üí /{phoneme['phoneme']}/ (GOP: {phoneme['gop_score']:.2f})\")\n",
    "    \n",
    "    # Word-level error analysis\n",
    "    if word_errors:\n",
    "        print(f\"\\nüó£Ô∏è WORD-LEVEL ERROR ANALYSIS\")\n",
    "        print(\"-\" * 35)\n",
    "        severe_word_errors = [we for we in word_errors if we['severity'] > 0.6]\n",
    "        \n",
    "        if severe_word_errors:\n",
    "            print(f\"üö® Words with pronunciation issues ({len(severe_word_errors)}):\")\n",
    "            for word_error in severe_word_errors:\n",
    "                error_types = \", \".join(word_error['error_types'])\n",
    "                print(f\"   '{word_error['word']}' - {error_types} (severity: {word_error['severity']:.2f})\")\n",
    "        else:\n",
    "            print(\"‚úÖ No significant word-level pronunciation errors detected!\")\n",
    "    \n",
    "    # Prosodic features analysis\n",
    "    print(f\"\\nüéµ PROSODIC FEATURES ANALYSIS\")\n",
    "    print(\"-\" * 34)\n",
    "    print(f\"üé∂ Intonation Variability: {prosodic_features['intonation_variability']:.2f}\")\n",
    "    print(f\"üîä Voice Quality Score: {prosodic_features['voice_quality_score']:.2f}/1.00\")\n",
    "    print(f\"ü•Å Rhythm Regularity: {prosodic_features['rhythm_regularity']:.2f}/1.00\")\n",
    "    print(f\"‚ö° Stress Points Detected: {prosodic_features['stress_points']}\")\n",
    "    print(f\"üéµ Pitch Range: {prosodic_features['f0_range']:.1f} Hz\")\n",
    "    \n",
    "    # Specific recommendations\n",
    "    print(f\"\\nüí° PERSONALIZED RECOMMENDATIONS\")\n",
    "    print(\"-\" * 35)\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    # Fluency recommendations\n",
    "    if fluency_assessment['speaking_rate_wpm'] < 120:\n",
    "        recommendations.append(\"üêå Try to speak a bit faster for better fluency\")\n",
    "    elif fluency_assessment['speaking_rate_wpm'] > 220:\n",
    "        recommendations.append(\"üèÉ Try to slow down slightly for clearer pronunciation\")\n",
    "    \n",
    "    if fluency_assessment['avg_pause_duration'] > 0.8:\n",
    "        recommendations.append(\"‚è∏Ô∏è Work on reducing pause length between words\")\n",
    "    \n",
    "    if fluency_assessment['pause_frequency'] > 2.0:\n",
    "        recommendations.append(\"üîÑ Practice smoother transitions between words\")\n",
    "    \n",
    "    # Pronunciation recommendations\n",
    "    if gop_scores and avg_gop < 0.7:\n",
    "        recommendations.append(\"üéØ Focus on individual phoneme clarity\")\n",
    "    \n",
    "    # Prosodic recommendations\n",
    "    if prosodic_features['intonation_variability'] < 0.1:\n",
    "        recommendations.append(\"üé∂ Add more intonation variation for natural speech\")\n",
    "    elif prosodic_features['intonation_variability'] > 0.5:\n",
    "        recommendations.append(\"üìà Work on controlling pitch variations\")\n",
    "    \n",
    "    if prosodic_features['rhythm_regularity'] < 0.4:\n",
    "        recommendations.append(\"ü•Å Practice maintaining consistent speech rhythm\")\n",
    "    \n",
    "    if not recommendations:\n",
    "        recommendations.append(\"üéâ Excellent pronunciation! Keep up the great work!\")\n",
    "    \n",
    "    for i, rec in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. {rec}\")\n",
    "    \n",
    "    # Progress tracking suggestion\n",
    "    print(f\"\\nüìà PROGRESS TRACKING\")\n",
    "    print(\"-\" * 20)\n",
    "    print(f\"üéØ Current Level: {fluency_level}\")\n",
    "    print(f\"üìä Key Metrics to Track:\")\n",
    "    print(f\"   ‚Ä¢ Fluency Score: {fluency_score:.2f}\")\n",
    "    print(f\"   ‚Ä¢ GOP Score: {avg_gop:.2f}\" if gop_scores else \"   ‚Ä¢ GOP Score: Not available\")\n",
    "    print(f\"   ‚Ä¢ Speaking Rate: {fluency_assessment['speaking_rate_wpm']:.1f} WPM\")\n",
    "    print(f\"   ‚Ä¢ Voice Quality: {prosodic_features['voice_quality_score']:.2f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Analysis complete! üéâ\")\n",
    "    print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f6b30598",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_phonemes(word, lexicon):\n",
    "    \"\"\"\n",
    "    Simple phoneme lookup function (alias for get_phonemes_enhanced for compatibility).\n",
    "    \"\"\"\n",
    "    return get_phonemes_enhanced(word, lexicon)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e38a131",
   "metadata": {},
   "source": [
    "## Day 5: System Integration and Feedback Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "974c3bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_gen_ielts(audio_path, lexicon):\n",
    "    \"\"\"\n",
    "    Enhanced main orchestrator for the GenIELTS system with advanced features.\n",
    "    \"\"\"\n",
    "    print(\"üéØ Starting Enhanced GenIELTS Analysis...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Day 1 & 3: Get transcription and aligned phonemes\n",
    "    actual_aligned_phonemes, transcription = get_phoneme_timings(audio_path, lexicon)\n",
    "    \n",
    "    if isinstance(actual_aligned_phonemes, str): # Error handling\n",
    "        print(actual_aligned_phonemes)\n",
    "        return\n",
    "\n",
    "    print(f\"üìù Transcription: {transcription}\")\n",
    "\n",
    "    # Day 2: Get expected phonemes using enhanced G2P\n",
    "    words = transcription.split()\n",
    "    expected_phonemes_list = [get_phonemes_enhanced(word, lexicon) for word in words]\n",
    "    expected_phonemes_str = \" \".join(expected_phonemes_list)\n",
    "    \n",
    "    print(f\"üî§ Expected Phonemes: {expected_phonemes_str}\")\n",
    "\n",
    "    # Day 4: Compare and find errors\n",
    "    errors = compare_phonemes(expected_phonemes_str, actual_aligned_phonemes)\n",
    "    \n",
    "    # NEW: Calculate GOP scores\n",
    "    print(\"\\nüéØ Calculating GOP Scores...\")\n",
    "    gop_scores = calculate_gop_scores(audio_path, expected_phonemes_str, actual_aligned_phonemes)\n",
    "    \n",
    "    # NEW: Map errors to words\n",
    "    print(\"üó∫Ô∏è Mapping errors to words...\")\n",
    "    word_errors = map_errors_to_words(errors, transcription, expected_phonemes_list)\n",
    "    \n",
    "    # NEW: Analyze prosodic features\n",
    "    print(\"üéµ Analyzing prosodic features...\")\n",
    "    prosodic_features = analyze_prosodic_features(audio_path)\n",
    "    \n",
    "    # NEW: Assess sentence-level fluency\n",
    "    print(\"üó£Ô∏è Assessing sentence fluency...\")\n",
    "    fluency_assessment = assess_sentence_fluency(audio_path, transcription, gop_scores, prosodic_features)\n",
    "    \n",
    "    # Enhanced feedback generation\n",
    "    generate_enhanced_feedback(errors, word_errors, gop_scores, prosodic_features, fluency_assessment, transcription)\n",
    "    \n",
    "    return {\n",
    "        'transcription': transcription,\n",
    "        'errors': errors,\n",
    "        'word_errors': word_errors,\n",
    "        'gop_scores': gop_scores,\n",
    "        'prosodic_features': prosodic_features,\n",
    "        'fluency_assessment': fluency_assessment\n",
    "    }\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "509659bd",
   "metadata": {},
   "source": [
    "## System Demonstration\n",
    "\n",
    "Let's test the complete GenIELTS system with the provided audio file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4852e1df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ ENHANCED GenIELTS Phoneme Error Detection System\n",
      "============================================================\n",
      "\n",
      "üé§ Analyzing audio file: test.wav\n",
      "-----------------------------------\n",
      "üéØ Starting Enhanced GenIELTS Analysis...\n",
      "==================================================\n",
      "Tokenizer error with 'TEN THINGS BRITISH PEOPLE SAY A LOT LET'S GO NUR TEN WE HAVE YOU'VE LOST THE PLOT YOU'VE LOST THE PLOT NU NINE WE HAVE MATE MATE ARE YOU RIGHT MATE MATE MATE MAN N EIGHT WE HAVE CHEERS CHEERS CHEERS NUMBER SEVEN DARLING DARLING YOU'RE RIGHT DARLING DARLING COME HERE PLEASE DARLING NUMBER SIX WE'VE GOT I'VE GOT THE RIGHT HUMP I'VE GOT THE RIGHT HUMP TO DAY NU FIVE WE HAVE ALL RIGHT YOU'RE RIGHT ARE YOU RIGHT ARE YOU ALL RIGHT NU FOUR WE HAVE HOW DO YOU DO HOW DO YOU DO HOW DO YOU DO NUR THREE WE HAVE SORRY SORRY SORRY NM TWO WE HAVE DO YOU KNOW WHAT I MEAN BUT YOU HAVE OSAY WIT TH ACTOR DO YOU KNOW WHAT I MEAN DO YOU NOWT WHANT I MAIN DOYOU NOW WHAT I A  ONE WE HAVE A FIT BIRD WE HAVE A BIRD OR THAT'S A FIT BIRD OR THAT'S A BIRD YOU'RE A B': 'T'\n",
      "Final waveform shape for FA model: torch.Size([1, 944774])\n",
      "Tokenizer error with 'TEN THINGS BRITISH PEOPLE SAY A LOT LET'S GO NUR TEN WE HAVE YOU'VE LOST THE PLOT YOU'VE LOST THE PLOT NU NINE WE HAVE MATE MATE ARE YOU RIGHT MATE MATE MATE MAN N EIGHT WE HAVE CHEERS CHEERS CHEERS NUMBER SEVEN DARLING DARLING YOU'RE RIGHT DARLING DARLING COME HERE PLEASE DARLING NUMBER SIX WE'VE GOT I'VE GOT THE RIGHT HUMP I'VE GOT THE RIGHT HUMP TO DAY NU FIVE WE HAVE ALL RIGHT YOU'RE RIGHT ARE YOU RIGHT ARE YOU ALL RIGHT NU FOUR WE HAVE HOW DO YOU DO HOW DO YOU DO HOW DO YOU DO NUR THREE WE HAVE SORRY SORRY SORRY NM TWO WE HAVE DO YOU KNOW WHAT I MEAN BUT YOU HAVE OSAY WIT TH ACTOR DO YOU KNOW WHAT I MEAN DO YOU NOWT WHANT I MAIN DOYOU NOW WHAT I A  ONE WE HAVE A FIT BIRD WE HAVE A BIRD OR THAT'S A FIT BIRD OR THAT'S A BIRD YOU'RE A B': 'T'\n",
      "Final waveform shape for FA model: torch.Size([1, 944774])\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[20], line 9\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m35\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Run the enhanced system\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mrun_gen_ielts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtest.wav\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbritfone_lexicon\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[12], line 9\u001b[0m, in \u001b[0;36mrun_gen_ielts\u001b[1;34m(audio_path, lexicon)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Day 1 & 3: Get transcription and aligned phonemes\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m actual_aligned_phonemes, transcription \u001b[38;5;241m=\u001b[39m \u001b[43mget_phoneme_timings\u001b[49m\u001b[43m(\u001b[49m\u001b[43maudio_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlexicon\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(actual_aligned_phonemes, \u001b[38;5;28mstr\u001b[39m): \u001b[38;5;66;03m# Error handling\u001b[39;00m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;28mprint\u001b[39m(actual_aligned_phonemes)\n",
      "Cell \u001b[1;32mIn[19], line 36\u001b[0m, in \u001b[0;36mget_phoneme_timings\u001b[1;34m(file_path, lexicon)\u001b[0m\n\u001b[0;32m     33\u001b[0m     expected_phonemes_list \u001b[38;5;241m=\u001b[39m [get_phonemes_enhanced(word, lexicon) \u001b[38;5;28;01mfor\u001b[39;00m word \u001b[38;5;129;01min\u001b[39;00m words]\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;66;03m# Use forced alignment for accurate word timestamps\u001b[39;00m\n\u001b[1;32m---> 36\u001b[0m     aligned_phonemes \u001b[38;5;241m=\u001b[39m \u001b[43mtransformer_forced_alignment\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     37\u001b[0m \u001b[43m        \u001b[49m\u001b[43mwaveform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtranscription\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexpected_phonemes_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwords\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlexicon\u001b[49m\n\u001b[0;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m aligned_phonemes, transcription\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "Cell \u001b[1;32mIn[19], line 133\u001b[0m, in \u001b[0;36mtransformer_forced_alignment\u001b[1;34m(waveform, transcription, expected_phonemes_list, words, lexicon)\u001b[0m\n\u001b[0;32m    131\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(tokens)\n\u001b[0;32m    132\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(emission, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m--> 133\u001b[0m         tokens \u001b[38;5;241m=\u001b[39m \u001b[43mtokens\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43memission\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    134\u001b[0m     alignments, scores \u001b[38;5;241m=\u001b[39m F_audio\u001b[38;5;241m.\u001b[39mforced_align(emission, tokens, blank\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    136\u001b[0m     \u001b[38;5;66;03m# Fallback: use CTC beam search decoder\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Test the enhanced GenIELTS system with test.wav\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ ENHANCED GenIELTS Phoneme Error Detection System\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüé§ Analyzing audio file: test.wav\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Run the enhanced system\n",
    "results = run_gen_ielts(\"test.wav\", britfone_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d07d1892",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "üéØ ENHANCED GenIELTS Phoneme Error Detection System\n",
      "============================================================\n",
      "\n",
      "üé§ Analyzing audio file: test.wav\n",
      "-----------------------------------\n",
      "üéØ Starting Enhanced GenIELTS Analysis...\n",
      "==================================================\n",
      "Tokenizer error with 'S GANTO SAY THESE WORDS IN AN ARABIC ACCENT READY OK WORLD WORLD NEXT NEKIST YOU TO U TUBE WINTER WINTER GOGLE GOGL PETER BETER HARRY POTTER HARRY BOTTER MANAGER MANAGER AIRPLANE AIRPLANE TRIP TRIB CERIAL CYRIL THAT'S IT THAT'S IT TIK TALK TTA': 'S'\n",
      "Final waveform shape for FA model: torch.Size([1, 779076])\n",
      "Tokenizer error with 'S GANTO SAY THESE WORDS IN AN ARABIC ACCENT READY OK WORLD WORLD NEXT NEKIST YOU TO U TUBE WINTER WINTER GOGLE GOGL PETER BETER HARRY POTTER HARRY BOTTER MANAGER MANAGER AIRPLANE AIRPLANE TRIP TRIB CERIAL CYRIL THAT'S IT THAT'S IT TIK TALK TTA': 'S'\n",
      "Final waveform shape for FA model: torch.Size([1, 779076])\n",
      "Forced alignment failed: targets Tensor shouldn't contain blank index. Found tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43], device='cuda:0').\n",
      "Debug info - transcription: 'S GANTO SAY THESE WORDS IN AN ARABIC ACCENT READY OK WORLD WORLD NEXT NEKIST YOU TO U TUBE WINTER WINTER GOGLE GOGL PETER BETER HARRY POTTER HARRY BOTTER MANAGER MANAGER AIRPLANE AIRPLANE TRIP TRIB CERIAL CYRIL THAT'S IT THAT'S IT TIK TALK TTA', tokens: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43], device='cuda:0')\n",
      "üìù Transcription: S GANTO SAY THESE WORDS IN AN ARABIC ACCENT READY OK WORLD WORLD NEXT NEKIST YOU TO U TUBE WINTER WINTER GOGLE GOGL PETER BETER HARRY POTTER HARRY BOTTER MANAGER MANAGER AIRPLANE AIRPLANE TRIP TRIB CERIAL CYRIL THAT'S IT THAT'S IT TIK TALK TTA\n",
      "üî§ Expected Phonemes: EH1 S G AE1 N T OW0  s Ààe…™  √∞ ÀàiÀê z  w Àà…úÀê d z  …™ n  …ô n  Àà√¶ …π …ô b …™ k  …ô k s Àà…õ n t  …π Àà…õ d i OW1 K EY1  w Àà…úÀê l d  w Àà…úÀê l d  n Àà…õ k s t N EH1 K IH0 S T  j …ô  t …ô  j ÀàuÀê  t j ÀàuÀê b  w Àà…™ n t …ô  w Àà…™ n t …ô G AA1 G AH0 L G AA1 G AH0 L  p ÀàiÀê t …ô B IY1 T ER0  h Àà√¶ …π i  p Àà…í t …ô  h Àà√¶ …π i B AA1 T ER0  m Àà√¶ n …™ d í …ô  m Àà√¶ n …™ d í …ô  Àà…õ…ô p l Àåe…™ n  Àà…õ…ô p l Àåe…™ n  t …π Àà…™ p T R IH1 B S EH1 R IY0 AH0 L S IH1 R AH0 L DH AE1 T S  …™ t DH AE1 T S  …™ t T IH1 K  t Àà…îÀê k T AA1\n",
      "\n",
      "üéØ Calculating GOP Scores...\n",
      "üó∫Ô∏è Mapping errors to words...\n",
      "üéµ Analyzing prosodic features...\n",
      "Forced alignment failed: targets Tensor shouldn't contain blank index. Found tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43], device='cuda:0').\n",
      "Debug info - transcription: 'S GANTO SAY THESE WORDS IN AN ARABIC ACCENT READY OK WORLD WORLD NEXT NEKIST YOU TO U TUBE WINTER WINTER GOGLE GOGL PETER BETER HARRY POTTER HARRY BOTTER MANAGER MANAGER AIRPLANE AIRPLANE TRIP TRIB CERIAL CYRIL THAT'S IT THAT'S IT TIK TALK TTA', tokens: tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
      "        18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35,\n",
      "        36, 37, 38, 39, 40, 41, 42, 43], device='cuda:0')\n",
      "üìù Transcription: S GANTO SAY THESE WORDS IN AN ARABIC ACCENT READY OK WORLD WORLD NEXT NEKIST YOU TO U TUBE WINTER WINTER GOGLE GOGL PETER BETER HARRY POTTER HARRY BOTTER MANAGER MANAGER AIRPLANE AIRPLANE TRIP TRIB CERIAL CYRIL THAT'S IT THAT'S IT TIK TALK TTA\n",
      "üî§ Expected Phonemes: EH1 S G AE1 N T OW0  s Ààe…™  √∞ ÀàiÀê z  w Àà…úÀê d z  …™ n  …ô n  Àà√¶ …π …ô b …™ k  …ô k s Àà…õ n t  …π Àà…õ d i OW1 K EY1  w Àà…úÀê l d  w Àà…úÀê l d  n Àà…õ k s t N EH1 K IH0 S T  j …ô  t …ô  j ÀàuÀê  t j ÀàuÀê b  w Àà…™ n t …ô  w Àà…™ n t …ô G AA1 G AH0 L G AA1 G AH0 L  p ÀàiÀê t …ô B IY1 T ER0  h Àà√¶ …π i  p Àà…í t …ô  h Àà√¶ …π i B AA1 T ER0  m Àà√¶ n …™ d í …ô  m Àà√¶ n …™ d í …ô  Àà…õ…ô p l Àåe…™ n  Àà…õ…ô p l Àåe…™ n  t …π Àà…™ p T R IH1 B S EH1 R IY0 AH0 L S IH1 R AH0 L DH AE1 T S  …™ t DH AE1 T S  …™ t T IH1 K  t Àà…îÀê k T AA1\n",
      "\n",
      "üéØ Calculating GOP Scores...\n",
      "üó∫Ô∏è Mapping errors to words...\n",
      "üéµ Analyzing prosodic features...\n",
      "üó£Ô∏è Assessing sentence fluency...\n",
      "\n",
      "============================================================\n",
      "üéØ ENHANCED GenIELTS PRONUNCIATION ASSESSMENT\n",
      "============================================================\n",
      "\n",
      "üìä OVERALL FLUENCY ASSESSMENT\n",
      "-----------------------------------\n",
      "üèÜ Fluency Score: 0.52/1.00 (Lower-Intermediate (IELTS 4-5))\n",
      "‚è±Ô∏è Speaking Rate: 54.2 WPM\n",
      "‚è∏Ô∏è Pause Analysis: 40 pauses, avg 0.53s\n",
      "üéØ Pronunciation Consistency: 1.00/1.00\n",
      "\n",
      "üéØ PRONUNCIATION QUALITY (GOP Analysis)\n",
      "------------------------------------------\n",
      "üìà Average GOP Score: 0.72/1.00\n",
      "\n",
      "üéµ PROSODIC FEATURES ANALYSIS\n",
      "----------------------------------\n",
      "üé∂ Intonation Variability: 1.64\n",
      "üîä Voice Quality Score: 0.21/1.00\n",
      "ü•Å Rhythm Regularity: 0.93/1.00\n",
      "‚ö° Stress Points Detected: 64\n",
      "üéµ Pitch Range: 2139.8 Hz\n",
      "\n",
      "üí° PERSONALIZED RECOMMENDATIONS\n",
      "-----------------------------------\n",
      "1. üêå Try to speak a bit faster for better fluency\n",
      "2. üìà Work on controlling pitch variations\n",
      "\n",
      "üìà PROGRESS TRACKING\n",
      "--------------------\n",
      "üéØ Current Level: Lower-Intermediate (IELTS 4-5)\n",
      "üìä Key Metrics to Track:\n",
      "   ‚Ä¢ Fluency Score: 0.52\n",
      "   ‚Ä¢ GOP Score: 0.72\n",
      "   ‚Ä¢ Speaking Rate: 54.2 WPM\n",
      "   ‚Ä¢ Voice Quality: 0.21\n",
      "\n",
      "============================================================\n",
      "Analysis complete! üéâ\n",
      "============================================================\n",
      "üó£Ô∏è Assessing sentence fluency...\n",
      "\n",
      "============================================================\n",
      "üéØ ENHANCED GenIELTS PRONUNCIATION ASSESSMENT\n",
      "============================================================\n",
      "\n",
      "üìä OVERALL FLUENCY ASSESSMENT\n",
      "-----------------------------------\n",
      "üèÜ Fluency Score: 0.52/1.00 (Lower-Intermediate (IELTS 4-5))\n",
      "‚è±Ô∏è Speaking Rate: 54.2 WPM\n",
      "‚è∏Ô∏è Pause Analysis: 40 pauses, avg 0.53s\n",
      "üéØ Pronunciation Consistency: 1.00/1.00\n",
      "\n",
      "üéØ PRONUNCIATION QUALITY (GOP Analysis)\n",
      "------------------------------------------\n",
      "üìà Average GOP Score: 0.72/1.00\n",
      "\n",
      "üéµ PROSODIC FEATURES ANALYSIS\n",
      "----------------------------------\n",
      "üé∂ Intonation Variability: 1.64\n",
      "üîä Voice Quality Score: 0.21/1.00\n",
      "ü•Å Rhythm Regularity: 0.93/1.00\n",
      "‚ö° Stress Points Detected: 64\n",
      "üéµ Pitch Range: 2139.8 Hz\n",
      "\n",
      "üí° PERSONALIZED RECOMMENDATIONS\n",
      "-----------------------------------\n",
      "1. üêå Try to speak a bit faster for better fluency\n",
      "2. üìà Work on controlling pitch variations\n",
      "\n",
      "üìà PROGRESS TRACKING\n",
      "--------------------\n",
      "üéØ Current Level: Lower-Intermediate (IELTS 4-5)\n",
      "üìä Key Metrics to Track:\n",
      "   ‚Ä¢ Fluency Score: 0.52\n",
      "   ‚Ä¢ GOP Score: 0.72\n",
      "   ‚Ä¢ Speaking Rate: 54.2 WPM\n",
      "   ‚Ä¢ Voice Quality: 0.21\n",
      "\n",
      "============================================================\n",
      "Analysis complete! üéâ\n",
      "============================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\abdal\\AppData\\Local\\Temp\\ipykernel_2684\\1220281670.py:39: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  'tempo': float(tempo),\n"
     ]
    }
   ],
   "source": [
    "# Test the enhanced GenIELTS system with test.wav\n",
    "print(\"=\" * 60)\n",
    "print(\"üéØ ENHANCED GenIELTS Phoneme Error Detection System\")\n",
    "print(\"=\" * 60)\n",
    "print(\"\\nüé§ Analyzing audio file: test.wav\")\n",
    "print(\"-\" * 35)\n",
    "\n",
    "# Run the enhanced system\n",
    "results = run_gen_ielts(\"artest.wav\", britfone_lexicon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e7531dd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "üîç DETAILED BREAKDOWN OF ENHANCED SYSTEM COMPONENTS\n",
      "======================================================================\n",
      "\n",
      "1. üìù ENHANCED TRANSCRIPTION:\n",
      "------------------------------\n",
      "Transcription: 'S GANTO SAY THESE WORDS IN AN ARABIC ACCENT READY OK WORLD WORLD NEXT NEKIST YOU TO U TUBE WINTER WINTER GOGLE GOGL PETER BETER HARRY POTTER HARRY BOTTER MANAGER MANAGER AIRPLANE AIRPLANE TRIP TRIB CERIAL CYRIL THAT'S IT THAT'S IT TIK TALK TTA'\n",
      "\n",
      "2. üéØ GOP (Goodness of Pronunciation) SCORES:\n",
      "-----------------------------------------------\n",
      "Top GOP scores (phoneme level):\n",
      "   EH1 ‚Üí  EH1 | GOP: 0.721 | Match: 1.00\n",
      "     S ‚Üí    S | GOP: 0.721 | Match: 1.00\n",
      "     G ‚Üí    G | GOP: 0.721 | Match: 1.00\n",
      "   AE1 ‚Üí  AE1 | GOP: 0.721 | Match: 1.00\n",
      "     N ‚Üí    N | GOP: 0.721 | Match: 1.00\n",
      "     T ‚Üí    T | GOP: 0.721 | Match: 1.00\n",
      "   OW0 ‚Üí  OW0 | GOP: 0.721 | Match: 1.00\n",
      "     s ‚Üí    s | GOP: 0.721 | Match: 1.00\n",
      "  ... and 165 more phonemes\n",
      "\n",
      "3. üó£Ô∏è WORD-LEVEL ERROR MAPPING:\n",
      "-----------------------------------\n",
      "‚úÖ No word-level errors detected!\n",
      "\n",
      "4. üéµ PROSODIC FEATURES:\n",
      "-------------------------\n",
      "  F0 Mean: 326.4 Hz\n",
      "  Intonation Variability: 1.643\n",
      "  Voice Quality: 0.207\n",
      "  Rhythm Regularity: 0.928\n",
      "  Detected Stress Points: 64\n",
      "\n",
      "5. üèÜ SENTENCE-LEVEL FLUENCY:\n",
      "------------------------------\n",
      "  Overall Score: 0.518/1.000\n",
      "  Speaking Rate: 54.2 WPM\n",
      "  Fluency Level: Lower-Intermediate (IELTS 4-5)\n",
      "  Pause Count: 40\n",
      "  Pronunciation Consistency: 1.000\n",
      "\n",
      "6. üîß TECHNICAL IMPROVEMENTS:\n",
      "--------------------------------\n",
      "‚úÖ Alternative G2P backend (no espeak dependency)\n",
      "‚úÖ GOP scoring for pronunciation quality\n",
      "‚úÖ Word-level error mapping\n",
      "‚úÖ Prosodic features analysis\n",
      "‚úÖ Sentence-level fluency assessment\n",
      "‚úÖ Enhanced feedback generation\n"
     ]
    }
   ],
   "source": [
    "# Enhanced detailed breakdown showing new features\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"üîç DETAILED BREAKDOWN OF ENHANCED SYSTEM COMPONENTS\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if 'results' in locals() and results:\n",
    "    # Step 1: Enhanced transcription\n",
    "    print(\"\\n1. üìù ENHANCED TRANSCRIPTION:\")\n",
    "    print(\"-\" * 30)\n",
    "    transcription = results.get('transcription', '')\n",
    "    print(f\"Transcription: '{transcription}'\")\n",
    "\n",
    "    # Step 2: GOP Scores\n",
    "    print(\"\\n2. üéØ GOP (Goodness of Pronunciation) SCORES:\")\n",
    "    print(\"-\" * 47)\n",
    "    gop_scores = results.get('gop_scores', [])\n",
    "    if gop_scores:\n",
    "        print(\"Top GOP scores (phoneme level):\")\n",
    "        for i, score in enumerate(gop_scores[:8]):  # Show first 8\n",
    "            print(f\"  {score['expected']:>4} ‚Üí {score['phoneme']:>4} | GOP: {score['gop_score']:.3f} | Match: {score['match_score']:.2f}\")\n",
    "        if len(gop_scores) > 8:\n",
    "            print(f\"  ... and {len(gop_scores) - 8} more phonemes\")\n",
    "    else:\n",
    "        print(\"GOP scores not available\")\n",
    "\n",
    "    # Step 3: Word-level errors\n",
    "    print(\"\\n3. üó£Ô∏è WORD-LEVEL ERROR MAPPING:\")\n",
    "    print(\"-\" * 35)\n",
    "    word_errors = results.get('word_errors', [])\n",
    "    if word_errors:\n",
    "        print(\"Words with pronunciation issues:\")\n",
    "        for error in word_errors:\n",
    "            print(f\"  '{error['word']}' - {', '.join(error['error_types'])} (severity: {error['severity']:.2f})\")\n",
    "    else:\n",
    "        print(\"‚úÖ No word-level errors detected!\")\n",
    "\n",
    "    # Step 4: Prosodic features\n",
    "    print(\"\\n4. üéµ PROSODIC FEATURES:\")\n",
    "    print(\"-\" * 25)\n",
    "    prosodic = results.get('prosodic_features', {})\n",
    "    if prosodic:\n",
    "        print(f\"  F0 Mean: {prosodic.get('f0_mean', 0):.1f} Hz\")\n",
    "        print(f\"  Intonation Variability: {prosodic.get('intonation_variability', 0):.3f}\")\n",
    "        print(f\"  Voice Quality: {prosodic.get('voice_quality_score', 0):.3f}\")\n",
    "        print(f\"  Rhythm Regularity: {prosodic.get('rhythm_regularity', 0):.3f}\")\n",
    "        print(f\"  Detected Stress Points: {prosodic.get('stress_points', 0)}\")\n",
    "\n",
    "    # Step 5: Fluency assessment\n",
    "    print(\"\\n5. üèÜ SENTENCE-LEVEL FLUENCY:\")\n",
    "    print(\"-\" * 30)\n",
    "    fluency = results.get('fluency_assessment', {})\n",
    "    if fluency:\n",
    "        print(f\"  Overall Score: {fluency.get('overall_fluency_score', 0):.3f}/1.000\")\n",
    "        print(f\"  Speaking Rate: {fluency.get('speaking_rate_wpm', 0):.1f} WPM\")\n",
    "        print(f\"  Fluency Level: {fluency.get('fluency_level', 'Unknown')}\")\n",
    "        print(f\"  Pause Count: {fluency.get('pause_count', 0)}\")\n",
    "        print(f\"  Pronunciation Consistency: {fluency.get('pronunciation_consistency', 0):.3f}\")\n",
    "\n",
    "else:\n",
    "    print(\"‚ùå Enhanced analysis results not available. Please run the system first.\")\n",
    "\n",
    "print(\"\\n6. üîß TECHNICAL IMPROVEMENTS:\")\n",
    "print(\"-\" * 32)\n",
    "print(\"‚úÖ Alternative G2P backend (no espeak dependency)\")\n",
    "print(\"‚úÖ GOP scoring for pronunciation quality\")\n",
    "print(\"‚úÖ Word-level error mapping\")\n",
    "print(\"‚úÖ Prosodic features analysis\")\n",
    "print(\"‚úÖ Sentence-level fluency assessment\")\n",
    "print(\"‚úÖ Enhanced feedback generation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "266d5476",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec674f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mfa-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
